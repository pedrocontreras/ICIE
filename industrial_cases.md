 |  Source |    Name    |    Year    | Reliability | Country    | Industry   | Description | Impact     | Action     | 
 | ------- | ---------- | ---------- | ----------- | ---------- | ---------- | ----------- | ---------- | ---------- | 
 | https://www.risidata.com/Database/Detail/baku-tbilisi-ceyhan-pipeline-explosion | Baku-Tbilisi-Ceyhan Pipeline explosion | 2008 | Confirmed | Turkey | Petroleum | Hackers shut down alarms, cut off communications and super pressurized the crude oil in the line which resulted in an explosion. | The explosion caused more than 30,000 barrels of oil to spill in an area above a water aquifer and cost BP and its partners $5 million a day in transit tariffs during the closure, according to communications between BP and its bankers cited in “The Oil Road,” a book about the pipeline. Some of the worst damage was felt by the State Oil Fund of the Republic of Azerbaijan, which lost $1 billion in export revenue while the line was shut down, according to Jamala Aliyeva, a spokeswoman for the fund.
 | https://www.risidata.com/Database/Detail/iranian-oil-terminal-offline-after-malware-attack | Iranian Oil Terminal offline after malware attack | 2012 | Confirmed | Iran | Petroleum | Iran has been forced to disconnect key oil facilities after suffering a malware attack. The computer virus is believed to have hit the internal computer systems at Iran’s oil ministry and its national oil company.Equipment on the Kharg island and at other Iranian oil plants has been disconnected from the net as a precaution. | Equipment on the Kharg island and at other Iranian oil plants has been disconnected from the net as a precaution after suffering a malware attack.
 | https://www.risidata.com/Database/Detail/german-steel-mill-cyber-attack | German Steel Mill Cyber Attack | 2014 | Confirmed | Germany | Metals | Multiple attackers used an advanced social engineering attack to gain access to the company network and then worked their way onto the control system network.  This resulted in an incident where a furnace could not be shut down in the regular way and the furnace was in an undefined condition which resulted in massive damage to the whole system.” | A furnace could not be shut down in the regular way and the furnace was in an undefined condition which resulted in massive damage to the whole system.”
 | https://www.risidata.com/Database/Detail/russian-based-dragonfly-group-attacks-energy-industry | Russian-Based Dragonfly Group Attacks Energy Industry | 2014 | Confirmed | United States | Power and Utilities | Dragonfly a group that has been operating since at least 2011, first started by targeting defense and aviation companies in the U.S. and Canada. In 2013, the group moved their focus into the U.S. and European energy firms. Dragonfly gains entry through these methods: a. spear phishing emails delivering malware. b. watering hole attacks that redirected visitors to energy industry-related websites hosting an exploit kit. c. infecting legitimate software from three different ICS (industrial control systems) equipment manufacturers. As of now Dragonfly’s main motive seems to be cyber-espionage, with a likelihood of sabotage in the future.| With a growing dependencies on energy, if Dragonfly were to take action with the information it has already been able to access, this group could do a lot of damage to the U.S. and Western Europe. A possible outcome from an attack on our utilities could cripple manufactures that supply their armies with food and other crucial items.
 | https://www.risidata.com/Database/Detail/u-2-spy-plane-caused-widespread-shutdown-of-u.s.-flights-report | U-2 spy plane caused widespread shutdown of U.S. flights: report | 2014 | Confirmed | United States | Transportation | U-2 Spy plane caused computer glitch that lead to flight delays for several airport, but originated in LAX airspace. | Caused delays at several other airports including McCarran International Airport in Las Vegas, which caused an inconvenience to tens of thousands of passengers who were arriving and departing from Los Angeles International Airport.
 | https://www.risidata.com/Database/Detail/after-godzilla-attack-u.s.-warns-about-traffic-sign-hackers | After ‘Godzilla Attack!’ U.S. warns about traffic-sign hackers | 2014 | Confirmed | United States | Transportation | In May, signs on San Francisco’s Van Ness Ave were photographed flashing “Godzilla Attack! Turn Back”, this the first of many hacks done across the country to road signs. | No known accidents were reported in the article, but hacking these road signs could have lead to many injuries since drivers will slow their vehicles down to be able to read these digital signs.
 | https://www.risidata.com/Database/Detail/public-utility-compromised-after-brute-force-hack-attack-says-homeland-secu | Public utility compromised after brute-force hack attack, says Homeland Security | 2014 | Confirmed | United States | Power and Utilities | Hackers took advantage of a weak password security system at a public utility in the U.S. which has not been identified. | At the time of the attack, mechanical devices were disconnected from the control system for scheduled maintenance, so no actual damaged had been done to the system or equipment. | ICS-CERT was able to work with the affected entity to put in place mitigation strategies and ensure the security of their control systems before there was any impact to operations.
 | https://www.risidata.com/Database/Detail/broadcast-storm-shuts-down-dcs-consoles | Broadcast Storm Shuts Down DCS Consoles | 1997 | Likely But Unconfirmed | Canada | Pulp and Paper | The facility lost communications to the operator consoles on a steam plant DCS.  The problem was believed to be caused by an incorrectly configured Windows 95 workstation in another mill area that generated high levels of broadcast packets. | The DCS had to be removed from the mill network, preventing process data from being transferred to the business systems. | Installation of separate subnets and VLANs for process control
 | https://www.risidata.com/Database/Detail/plc-password-change | PLC Password Change | 1988 | Confirmed | Canada | Pulp and Paper | One of the first reported cases of plant floor “hacking” occurred in 1988 on an Allen-Bradley DH+, which was used by an angry worker to modify a different department’s PLC-5. The individual changed the password to something obscene, blocking all maintenance access to the system. It was believed that the employee found the original password on a post-it note. | Shut down of PLC to clear memory and reload program. Loss of staff time and a brief system shutdown while resetting the password.
 | https://www.risidata.com/Database/Detail/duplicate-ip-address-prevents-machine-startup | Duplicate IP Address Prevents Machine Startup | 1996 | Likely But Unconfirmed | Canada | Pulp and Paper | The mill had upgraded the profile controller on the #1 Paper Machine to a control system that used Ethernet and TCP/IP to communicate between the scanners and the main controller.  It was also connected to the main mill network through a bridge so that profile information could be accessed by business applications.  Some time after the installation, a network printer in another area of the mill was accidentally given the same IP address as the controller.  Initially this did not cause difficulties, but shortly after a routine maintenance shutdown, the scanners started directing their data to the printer rather than to the controller. | The paper machine could not be started for over six hours. | Installed VLANs and separate subnets between business and process networks
 | https://www.risidata.com/Database/Detail/whitehat-takeover-of-dcs-consoles | Whitehat Takeover of DCS Consoles | 2002 | Invalid | Canada | Petroleum | A simulated attack on a DCS during a security audit results in complete administrative takeover of the DCS operator consoles. A whitehat hacker with network access to the control LAN was to connect to selected DCS operator stations and obtain full administration privileges. This was accomplished through the vulnerabilities in the Windows platform and a number of Netbios fileshares that lacked proper password protection. | None | Changes to policy for configurations of all computers used in process control networks
 | https://www.risidata.com/Database/Detail/hackers-target-cal-iso-system | Hackers Target Cal - ISO System | 2001 | Confirmed | United States | Power and Utilities | Like the Salt River Project incident, this incident appears to be rampant with conflicting information. The best data we have is that a relatively inexperienced hacker was able to exploit two Solaris servers that were were part of a development network at Cal-ISO. These servers were supposed to be protected by a firewall, but in reality the servers were connected directly to the Internet. In addition, the Cal-ISO system administrators left the servers with all the software installed by the default setup, leaving numerous vulnerabilities open to exploitation. (#1)	The original LA-Times article of states:	“An internal agency report, stamped “restricted,” shows that the attack began as early as April 25 and was not detected until May 11. The report says the main attack was routed through China Telecom from someone in Guangdong province in China. In addition to using China Telecom, hackers entered the system by using Internet servers based in Santa Clara in Northern California and Tulsa, Okla., the report says. James Sample, the computer security specialist at Cal-ISO who wrote the report, said he could not tell for certain where the attackers were located.” (#2)	The system also lacked the ability to collect a record of events in a secure place, instead leaving them on the computers that the intruder could access. The investigators could not easily detect which files had been changed. A rudimentary root kit—a tool set used by Internet attackers to take total control of a system—had been installed, but other details could not be discovered. | “There was an obvious attempt made to penetrate our systems,” said Greg Fishman, spokesman for Cal-ISO, who would not give any more details. “They were able to achieve minimal penetration into a system that we use to demonstrate software. This was never a threat to our core operations.” (#1)
 | https://www.risidata.com/Database/Detail/electronic-sabatoge-of-venezuela-oil-operations | Electronic Sabotage of Venezuela Oil Operations | 2002 | Confirmed | Venezuela | Petroleum | In December 2002, PDVSA, Venezuela’s state oil company became embroiled in a bitter strike that saw extensive sabotage. According to a report in Oil Daily, Ali Rodriguez (the head of the oil company) stated:	“[...] we have suffered many acts of sabotage at the terminals, the refiners, and even to some well-heads in Lake Maracaibo.  There were even instances of computer hacking which did a lot of damage since much of the operation is centrally controlled by computer.”	Someone, possibly an employee involved in the general strike,  remotely accessed a program terminal to erase all PLC programs in port facility. | This and other physical sabotage apparently cut Venzuela's national production down to to 370,000 barrels per day, compared with 3 million barrels before the strike.	Eight hours loss of production.	Unable to load product into waiting tankers.
 | https://www.risidata.com/Database/Detail/nimda-impact-on-manufacturing-system | Nimda Impact on Manufacturing System | 2001 | Confirmed | United States | Food & Beverage | A  major manufacturing company that had implemented a complete anti-virus program for their IT environment.  All client hardware was required to have a specific anti-virus program with up-to-date signature files.  The IT servers were likewise protected.  In addition access to the IT systems was restricted to approved gateways and firewalls.  	The process control environment was under the control of a different department, which did not implement anti-virus protection or controlled access.  Late one night an control engineer, working on a production problem, logged in from home.  He dialled directly into one of the process control servers, something that had been done countless times before.  But in this particular case his personal PC at home had been infected with the Nimda virus.  It did not take long for the virus to hop across the production environment that was not prepared for this type of threat. | Herculean efforts of the IC and IT staff prevented a production stoppage at this facility, but the recovery effort cost thousands of dollars in staff time due to a lack of preparedness.  The virus exploited the common technology, servers and protocol that in the past were not present. | Change virus and remote access policy on plant floor.
 | https://www.risidata.com/Database/Detail/cia-trojan-causes-siberian-gas-pipeline-explosion | CIA Trojan Causes Siberian Gas Pipeline Explosion | 1982 | Likely But Unconfirmed | Russian Federation | Petroleum | Thomas Reed, senior US national security official, claims in his book “At The Abyss” that the United States allowed the USSR to steal pipeline control software from a Canadian company. This software included a Trojan Horse that caused a major explosion of the Trans-Siberian gas pipeline in June, 1982. The Trojan ran during a pressure test on the pipeline but doubled the usual pressure, causing the explosion. (#1, #2)	“In order to disrupt the Soviet gas supply, its hard currency earnings from the West, and the internal Russian economy, the pipeline software that was to run the pumps, turbines, and valves was programmed to go haywire, after a decent interval, to reset pump speeds and valve settings to produce pressures far beyond those acceptable to pipeline joints and welds,” Reed writes. (#3)	The scheme to plant bugs in Soviet software was masterminded by Gus Weiss, who at the time was on the National Security Council and who died last year. Soviet agents had been so keen to acquire US technology, they didn’t question its provenance. (#4)	Russian newspaper sources deny the report, saying an explosion did take place, but it was caused by poor construction, not by planted software. “What the Americans have written is rubbish,” said Vasily Pchelintsev, who in 1982 headed the KGB office in the Tyumen region, the likely site of the explosion described in the book.” (#5) | The software sabotage had two effects, explains Reed. The first was economic. By creating an explosion with the power of a three kiloton nuclear weapon, the US disrupted supplies of gas and consequential foreign currency earnings. But the project also had important psychological advantages in the battle between the two superpowers.	“By implication, every cell of the Soviet leviathan might be infected,” Reed writes. “They had no way of knowing which equipment was sound, which was bogus. All was suspect, which was the intended endgame for the entire operation.”
 | https://www.risidata.com/Database/Detail/hacker-takes-over-russian-gas-system | Hacker Takes Over Russian Gas System | 1999 | Likely But Unconfirmed | Russian Federation | Petroleum | According to a Associated Press news release,  in 1999 a hacker took over control of a Russian gas system by penetrating the Gazprom (Russia’s state-run gas monopoly) SCADA system.(#1). National Petroleum Council report seems to confirm this, but it may just be repeating the AP story.	AP story states:  “By acting with a Gazprom insider, hackers were able to get past the company’s security and break into the system controlling gas flows in pipelines, Interior Ministry Col. Konstantin Machabeli said, according to the Interfax news agency. The “central switchboard of gas flows” was "for some time” under the control of external users, Machabeli said in the report. He did not say if the hackers caused any damage. Machabeli said the hackers used a "Trojan horse” program, which stashes lines of harmful computer code in a benign-looking program, Interfax said. The report did not identify the hackers or say if anyone had been apprehended.”(#1)	However 1 week later the Russian Interior Ministry denied the report:	“Wednesday denied the Russian gas monopoly Gazprom's computer networks had been hacked. Deputy head of the Interior Ministry’s directorate for combatting high-tech crime, Konstantin Machabeli, told a press conference that reports about hacked Gazprom computer networks carried by some media were “slightly exaggerated”. At the same time, he admitted that his directorate had registered an attempt to hack Gazprom's gas flow control system. However, the company technological security service “managed to track them down and catch quickly”. The security service set up a trap computer to imitate Gazprom computer network’s operation. The hackers took the bait and broke into the computer.” (#3) | Loss of control of the SCADA system
 | https://www.risidata.com/Database/Detail/iranian-hackers-attempt-to-disrupt-israel-power-system | Iranian Hackers Attempt to Disrupt Israel Power System | 2003 | Likely But Unconfirmed | Israel | Power and Utilities | We have little information on this incident. According the the press report, Iranian hackers attempted to break into the Israel Electric Corporation’s computers and disrupt the power supply in Israel several times in recent months. The Iranians, some of whom may be university students, tried to damage the computer infrastructures operating Israel’s power stations. They sent viruses and attempted to overload the servers in a “denial of service” attack	Israel Electric Corp. spokesman Dedi Golan said the company is not releasing any information on the matter and that all the information is being transferred to the authorized bodies. | The Israel Electric Corp. managed to identify the hacking attempts and foiled them without any damage to the power systems. (#1)
 | https://www.risidata.com/Database/Detail/virus-impacts-paper-machine-hmi | Virus Impacts Paper Machine HMI | 2004 | Likely But Unconfirmed | United States | Pulp and Paper | A virus infects a PC running HMI software for controlling the operation of an manufacturing press. The virus introduced by a contractor accessing the HMI for maintenance via a remote dial-up line.
 | https://www.risidata.com/Database/Detail/ip-address-change-shuts-down-chemical-plant | IP Address Change Shuts Down Chemical Plant | 2002 | Confirmed | United States | Chemical | On March 4, 2002, the control room operator’s LAN computer was restarted with a changed IP address.  The IP address duplicated the address assigned to an analyzer computer used for continuous emissions monitoring.  The analyzer computer locked-up as a result of the network error message due to duplicate IP addresses.  	At the time of the incident the analyzer computer was not isolated from the plant network by a firewall.  While the individual responsible for changing the IP address was not identified officially, it was reported that the address was changed to enable Internet access on the control room operators PC to play games. | The loss of signal from the analyzer computer forced a plant shutdown until the network communication problem was resolved 2 hours later.
 | https://www.risidata.com/Database/Detail/hackers-attack-nz-aust-for-joining-gulf-taskforce | Hackers Attack NZ & Aust for Joining Gulf Taskforce | 1998 | Hoax / Urban Legend | New Zealand | Power and Utilities | The following FICTITIOUS news paper article opened Parliament of Australian Research Paper 18 1997-98: 	Hackers Attack NZ & Aust for Joining Gulf Taskforce	AZP London: A hacker group calling themselves the ‘Anti-Christ Doom Squad’ was involved in attacks against New Zealand and Australia just days after Wellington and Canberra announced troop deployments to the latest Gulf Crisis.	In a secret UK Government Communications Headquarters (GCHQ) report leaked today, the Auckland blackouts that crippled the city for weeks earlier in the year, were traced to electronic attacks on New Zealand’s electricity distribution network, launched by computers in Amsterdam over the Internet. A senior government source in the Australian equivalent to GCHQ, the Defence Signals Directorate (DSD), confirmed that the widespread blackouts across the Australian state of Queensland were also traced to the same source.	The 'Anti-Christ' hackers traversed computer systems worldwide using ‘spoofed' user-names and stolen passwords to try to conceal their identity. Once inside the New Zealand power companies' supercomputer, the hackers accessed a control system commonly used in energy distributions systems to launch their attack. The Supervisory Control and Data Acquisition (SCADA) system controls switches and flows across most modern power distribution networks.	The 'Anti Christ Doom Squad' then concentrated on manipulating one key choke-point on the outskirts of Auckland. It was the location where all five main power lines converged before entering the city. The ‘Doom Squad' altered the temperature within the gas-encased power lines thereby crippling them within minutes. The whole operation was launched and conducted from a drug caf | While Auckland did suffer from severe power blackouts in February 1998, this incident is NOT factual.
 | https://www.risidata.com/Database/Detail/theft-of-relay-programming-laptops | Theft of Relay Programming Laptops | 2004 | Invalid | United States | Power and Utilities | SUBJ:  COMPUTER THEFT; From: ESISAC; Date:  15 March 2004; Incident Report:On or about the morning of Tuesday, March 2, 2004 person(s) yet unknown broke into offices located at (xxx location) and stole two laptop computers that were programmed to perform station relay testing. These two laptops contained the various relay vendor softwares. These relay softwares are common industry wide in North America. The software is also readily available from relay vendors. It is important to understand that you cannot log on to the two computers to run any software unless you run a “crack program”, or have a valid user account on the PC and know the password. Having said that, a motivated hacker could crack the program and “plug and play” at any transformer station should they also break into a station’s relay room and have system knowledge of where and how to “plug and play”.  There is no remote access capabilities. What makes this break-in and theft suspicious as to motive is that there were a number of other items that could be easily “fenced” as stolen property left in the open and only these two laptops were touched. This may or may not be a mere coincidence. Local police were notified and (xxx location)‘s controlling authority was advised to be aware of any unusual relay activity. The (yyy location) was also made aware of this incident. To date, no unusual relay activity has been observed.
 | https://www.risidata.com/Database/Detail/scada-ems-alarm-system-failure-contributes-to-blackout | SCADA/EMS Alarm System Failure Contributes to Blackout | 2003 | Confirmed | United States | Power and Utilities | According to the U.S.-Canada Power System Outage Task Force Report, a significant contributing factor to the August 14th blackout was the failure of the First Energy (FE) Energy Management System (EMS) Alarm system and the operators unawareness of this critical failure. The details from the report are as follows:FE | The August 14th, 2003 Northeast blackout (although is was only one of number of causes).
 | https://www.risidata.com/Database/Detail/reverse-osmosis-system-plc-attacked | Reverse Osmosis System PLC Attacked | 2002 | Likely But Unconfirmed | United States | Electronic Manufacturing | A PLC operating a reverse osmosis system for water purification in a semiconductor manufacturing plant was attacked and disabled by unknown Internet hackers. The PLC was improperly protected as it was connected to a LAN directly accessible to the Internet | Loss of the reverse osmosis water system for several hours, but no economic loss as there was sufficient backup water. | PLC was moved to a firewall protected LAN
 | https://www.risidata.com/Database/Detail/dos-attack-shuts-down-port-of-houston | DoS Attack Shuts Down Port of Houston | 2001 | Confirmed | United States | Transportation | Aaron Caffrey, 19, was accused of bringing computers to a standstill at the port of Houston in Texas - but was found not guilty by a jury on October 17, 2003. This was despite both the prosecution and defence agreeing that Caffrey’s machine was responsible for launching the attack, that a list of 11,608 IP addresses of vulnerable servers was found on his hard drive, and the discovery of a malicious script on his system signed by someone called “Aaron”.  An investigation by US authorities traced the attack back to a computer at Caffrey’s home in Shaftesbury, Dorset. Investigators found a copy of the attack script on the computer. | Computers at the port suffered a severe denial-of-service attack on 20 September, 2001. The attack crashed systems at the port which provide crucial data for shipping pilots, mooring companies and support firms responsible for helping ships to navigate in and out of the harbour, placing shipping at risk.
 | https://www.risidata.com/Database/Detail/navy-radar-shuts-down-scada-systems | Navy Radar Shuts Down SCADA Systems | 1999 | Confirmed | United States | Water/Waste Water | In November 1999, the US Navy was conducting exercises off San Diego during which two commercial spectrum users experienced severe electro magnetic interference (EMI) to their Supervisory Control and Data Acquisition (SCADA) wireless networks operating at approximately 928.5 MHZ. | The San Diego County Water Authority (SDCWA) and the San Diego Gas and Electric (SDGE) Companies were unable to remotely actuate critical value openings and closings as a result.  This necessitated sending technicians to remote locations to manually open and close water and gas valves. | The cause of the EM interference was determined to be a NAVY AN/SPS 49 radar operating off the coast of San Diego.  Resulted in restrictions in Radar operation.
 | https://www.risidata.com/Database/Detail/hackers-crash-controller-via-web-service | Hackers Crash Controller via Web Service | 2002 | Confirmed | United Kingdom | Electronic Manufacturing | There were 2 types of incidents occurring at the same location in the same time frame: (1)  Hackers opened connections with our device , sent messages of unknown nature, then stopped communicating without closing the connection.  This exposed a bug in the control product.  After timing out the connection, the connection was left open.  After repeated attacks, all connections were consumed leaving the ethernet port unavailable for legitimate users. (2)  Hackers sent a WEB page to the device containing some Java script and the text: ” Hello! Welcome to http://worm.com Hacked by Chinese”.  This exposed a bug in TCP/IP stack that caused invalid data access abort in the CPU.  This results in the controller going through a reset, which drives all outputs to their off state. | Two engineers worked on this problem full time for 3-4 weeks each.  The hard part was being able to identify the cause so we could reproduce it at the vendor’s site.  Our internal firewalls greatly reduce the likelihood of these attacks getting through.  The customer in this case is a development partner, so we worked with them to capture the activity with a network analyzer.  Once the cause(s) were identified, the fixes were easy. | (1)  Fixed the bug in the device code - it now properly closes timeout connections. (2)  Contacted the TCP stack vendor who gave us a fix for his bug.  In addition, we created a tool to cause problem #1.  It rapidly makes connections but does not close them.  With this tool, the old "buggy" code locks up the Ethernet port within seconds.  The fixed code does not fail. Vendor strongly recommends that our customers work with their IT departments to isolate the controllers from these kinds of attacks.
 | https://www.risidata.com/Database/Detail/anti-virus-software-prevents-boiler-safety-shutdown | Anti-Virus Software Prevents Boiler Safety Shutdown | 2001 | Likely But Unconfirmed | United States | Petroleum | A TUV approved boiler safety protection system used Microsoft Excel on a PC workstation for programming. This workstation also had Norton anti-virus software running. The AV software prevented the proper communications between the PC and the protection system. | The protection system was incorrectly configured and a safety shutdown that should have occurred did not.
 | https://www.risidata.com/Database/Detail/telco-shuts-off-critical-scada-comms | Telco Shuts Off Critical SCADA Comms | 2003 | Confirmed | Canada | Petroleum | Staff working for a telecommunications provider accidentally shut a critical circuit for communications between the SCADA operations centre and field stations on a liquid pipeline. | SCADA operations was required to dispatch field crews to manually operate the field stations.
 | https://www.risidata.com/Database/Detail/slammer-infected-laptop-shuts-down-dcs | Slammer Infected Laptop Shuts Down DCS | 2003 | Confirmed | United Kingdom | Petroleum | A corporate laptop user installed software, unaware that it included an unpatched version of msSQL. Sometime later the user connected PC to the Internet (in violation of  company policy) to access email via an ISP. SQLslammer infected the internet connected machine. The user then brought the infected machine into the office and connected to the network, causing a small outbreak of the SQLslammer worm within the corporate network. An unfirewalled data acquisition server, a control system and a development control system became infected with SQLslammer worm and had to be removed from the network to prevent further infection. | The was no significant impact to production, but some history data was lost during server down-time. Data had to be manually created. | The company has a significant programme to raise awareness of digital security issues and implement security improvements. The servers have been patched and a firewall is being recommended for installation.
 | https://www.risidata.com/Database/Detail/blaster-impacts-hmi-stations-in-smelter | Blaster Impacts HMI Stations in Smelter | 2003 | Confirmed | Canada | Metals | HMIs process LAN infected with Blaster Worm. | Slowed down HMI response and required several man-weeks to clean up | Nothing
 | https://www.risidata.com/Database/Detail/hacker-changes-chemical-plant-set-points-via-modem | Hacker Changes Chemical Plant Set Points via Modem | 2002 | Confirmed | Canada | Chemical | Operators noticed set points being changed caused by a dial-up modem that was continuously plugin and active. | None | Disconnected modem and a set up access policy for modems
 | https://www.risidata.com/Database/Detail/proposed-hack-of-uk-water-systems | Proposed Hack of UK Water Systems | 2003 | Invalid | United Kingdom | Water/Waste Water | Zipser’s talk on water management explained various remote telemetry and management systems and their interconnects. Packet radio and PSTN are the most common methods, since cabling is expensive but other methods are on their way. (#1) None of the links perform encryption, making the only security that provided by obscurity of the (proprietary) protocols. (#1) Detailed breakdown of RF systems that are used by water management authorities in the UK and how these systems can be accused, interfered with and generally messed up. (#2) Live demo on how to monitor the un-encrypted water management systems and create a denial of service attack. Also showed that additional communication channels using dial up connections would kick in automatically in the event of such an attack. (#2)
 | https://www.risidata.com/Database/Detail/air-traffic-radar-system-in-palmdale-california-crashes | Air Traffic Radar System in Palmdale California Crashes | 2004 | Confirmed | United States | Transportation | A computer glitch, that was discovered more than a year earlier, caused  the backup system to fail on the FAA computer. This caused a 3 hour shutdown of radar systems at the Los Angeles Air Route Traffic Control Center in Palmdale California. 	The Voice Switching and Control System (VSCS), originally on Linux, upgraded about a year ago to Dell computers using Microsoft software. The Microsoft software contained an internal clock designed to shut the system down after 49.7 days to prevent it from becoming overloaded with data. Analysts said a shutdown mechanism is preferable to allowing an overloaded system to keep running and potentially give controllers wrong information about flights. The agency’s radio system in Palmdale shut itself down on the afternoon of Tuesday September 16, 2004 because a technician failed to reset and internal clock - a routine maintenance procedure required every 30 days by the FAA. Then a backup system failed, also as a result of technician error, officials said. (#1) | The radio failure rippled throughout the nation’s airports, grounding hundreds of commercial flights and forcing controllers working from other centers to divert hundreds more to locations outside Southern California. Los Angeles International Airport officials said about 30,000 passengers were affected, with 500 or spending the night in the terminals. The backlog of incoming flights was not cleared until 3am Wednesday. At LAX, 450 flights were diverted or cancelled and another 150 were delayed. An additional 32 were cancelled Wednesday morning because the aircraft did not arrive Tuesday night. 	Other airports - Ontario, Bob Hope, John Wayne, Long Beach and Palm Springs, as well as San Diego - experienced significant delays, and airports throughout the West took diverted flights. 	There was a loss of radar system for 178,000 square miles, throughout California, to Arizona and Nevada. (#1) | The computer glitch that snarled air traffic was first discovered more than a year earlier in Atlanta after the FAA upgraded its computers. However, the problem so far has been corrected only in Seattle, one of 21 FAA regional air-traffic control centers that have used the system since the mid-1990s. (#1)
 | https://www.risidata.com/Database/Detail/utility-scada-system-attacked | Utility SCADA System Attacked | 2001 | Likely But Unconfirmed | United States | Power and Utilities | An electric power utility allowed a contracted vendor to establish a VPN connection. Neither took adequate steps to ensure proper access protection thinking that the other had.  The connection was originally intended to have minimal exposure to the internet when in fact it had significant exposure.  A threat agent exploited this vulnerability allowing penetration of the SCADA system. (#2) | The attack resulted in significant financial impact to the utility even though they did not lose electric power and their customers were not physically affected. The utility lost use of its SCADA system for 2 weeks until the SCADA system could be completely reprogrammed and made a “trusted” system. The cost was 4 person-months of effort. The utility did not report the incident - there was no requirement, since no electric power was lost. (#2)
 | https://www.risidata.com/Database/Detail/virus-attacks-a-european-utility | Virus Attacks a European Utility | 2003 | Likely But Unconfirmed | Europe | Power and Utilities | A European utility reported that a virus attacked their Distribution SCADA system and this resulted in partial unavailability of the system functions. The utility reported they lost complete view of numerous distribution substations (for 3 days) by the operators in the control center. (#2) | It took approximately 40 person-weeks (over a 4 calendar-week period) to correct the problem. Since there was no electric power lost there was no requirement for the utility to report the incident. (#2)
 | https://www.risidata.com/Database/Detail/uk-maritime-and-coastguard-agency-hit-by-sasser-worm | UK Maritime and Coastguard Agency Hit by Sasser Worm | 2004 | Confirmed | United Kingdom | Other | The Sasser bug affected coastguard logging operations but left its command and control systems and lifesaving equipment unaffected. 	German student, Sven Jaschan, has been formally charged with computer sabotage, data manipulation and disruption of public systems by German prosecutors. | The Sasser worm has hit all 19 coastguard stations and the service’s headquarters, leaving officers reliant on pens and paper. According to the UK Coastguard there was no danger to the public - its computer mapping facilities were not working, but staff were still able to use a paper based system known as “Pinkies”. The Sasser bug has affected coastguard logging operations but left its command and control systems and lifesaving equipment unaffected. Coastguard staff were still able to use telephones and radios, but fax and telex machines were put out of action. (#1)
 | https://www.risidata.com/Database/Detail/sasser-worm-hits-british-airways | Sasser Worm Hits British Airways | 2004 | Confirmed | United Kingdom | Transportation | Between 0815 and 0830 BST on May 3, 2004, the Sasser worm infected the systems controlling the check-in, baggage handling and boarding at British Airways and caused the lost of use of around half its check-in desk computers at Heathrow’s Terminal Four. | Twenty British Airways flights were each delayed about 10 minutes due to Sasser troubles at check-in desks. A total of 21 flights to Israel, the Middle East, the US and East Africa were affected. The terminal was back to its normal timetable by 1200 BST. (#1)
 | https://www.risidata.com/Database/Detail/nachi-worm-on-advanced-process-control-servers | Nachi Worm on Advanced Process Control Servers | 2003 | Confirmed | France | Chemical | Staff noticed that the Advanced Process Controls (APC) Servers were getting slower and slower. Investigators found Nachi virus on 8 APCs (running Windows 2000) and disconnected these servers from production network for 5 hours.  The virus was not detected on the Honeywell DCS stations (running Windows NT). | Incident reduced feed for several hours (Running in a safer mode). | Changed firewall rules, changed policy, antivirus policy. The company will also be starting measures to better separate the APC Network from central business network.
 | https://www.risidata.com/Database/Detail/oil-company-scada-system-impacted-by-rf-interference | Oil Company SCADA System Impacted by RF Interference | 1989 | Likely But Unconfirmed | United States | Petroleum | In 1989 a SCADA system was being prepared for an oil company in Houston Texas. All the remote telemetry units were communicating with the master station computer via low power Johnson radios. The dummy loads on all of the antennae were used to cut down the range of the transmissions, (this caused havoc with the SWR’s and other equipment but the transceivers could be adjusted to get decent communications most of the time). Sporadically there would be bursts of errors for seemingly no reason. Using data analysers, junk could be seen on the frequency but could not be identified. Using a telephone handset on a circuit to listen to the 'noise'  it was established that a delivery truck was talking to his dispatcher at the same time that the communications efficiency dropped to zero. (#1) | Unknown. | Testing of the equipment was continued with the occasional communications breakdown.
 | https://www.risidata.com/Database/Detail/blockage-of-12-out-of-13-plc-systems | Blockage of 12 Out Of 13 PLC Systems | 2004 | Confirmed | Switzerland | Other | Two out of the 13 PLCs of the control system for the plant lost communications, with 10 PLCs more following in the next 30 minutes after the start of the incident. The corresponding modules (Siemens CP443-1 IT) were completely blocked, such that only a power-cycle was able to restart them. Since then, drop-outs of the same kind occur with a frequency of one PLC per week on average. Suspected sources for this incident have been network broadcasts or network scans, but no proof has been found in subsequent network sniffing. | The loss of a PLC inhibits the proper  functioning of the facility. In worst cases, this can lead to production losses of up to 8 hours per incident, depending on the status of the process. | The future plans are to put the whole system on a dedicated (virtual?) network. This move has not been accomplished at the time of this entry.
 | https://www.risidata.com/Database/Detail/weekly-connection-loss-to-plcs | Weekly Connection Loss to PLCs | 2004 | Confirmed | Switzerland | Other | Occasional communication connection loss  to the PLC hindered the smooth running of the temperature monitoring system. The affected Siemens PLC (CP343-1) needed to be power cycled to restore its operation. The frequency has been about once a week without any regularity. Current suspect is a malformed packet accepted by the PLC, but this has never been confirmed. | Loss of control on the production facility. | The modules were exchanged and upgraded to the most recent firmware version after confirmation with Siemens. The problem has been solved by this.
 | https://www.risidata.com/Database/Detail/instability-of-the-osi-layer-2-bridging | Instability of the OSI Layer-2 Bridging | 2004 | Confirmed | Switzerland | Other | The current campus network infrastructure allows bridging of non-IP protocols (ISO, AIX,..) in parallel to the IP protocols by using dedicated virtual networks. For the ISO-protocol (OSI layer-2) a loop-less architecture is automatically configured by the corresponding switches, with one switch as master. The master is auto-negotiated by determining the switch with the highest (?) MAC address plus the highest internal “administrator level” (default is 128; master is 240; maximum is 255, but however, not compatible with all switch models). A watchdog signal issued every 45s verifies the proper functioning of the master. A change of master takes about 30s for all switches to adjust themselves to the new master configuration. No other traffic is exchanged during that time. 	At the time of the incident, an old (2yrs) Cisco switch was master. Because of increased traffic in a remote part of the production network, a hub had been replaced by a new switch. The MAC address of this switch was rather high, such that it became number two in the hierarchy of the master switches. The Cisco switch was still number one. Due to an instability of the later ( i.e. its alive signals  were not strong enough to be received by the new switch, which was far away), the new switch took over mastership, but was not able to determine the full network topology. Thus, it went to “listen”-mode, during which the Cisco switch took over again. This swapping of mastership continued and blocked all other traffic, because of the re-configuration latency, until the problem has been isolated and fixed. | The traffic outage blocked the control of the important production facilities for several hours. | A temporary solution has been the addition of a second dedicated switch near by (and thus being able to detect the Cisco switch) with a high MAC address and an "administration level", such that it will definitively take over from the Cisco switch in case of its failure. In addition, external production switches are denied access in the auto-negotiation task. A final solution will be the exclusion of all non-IP protocols from the campus network, which might come by the end of 2004.
 | https://www.risidata.com/Database/Detail/change-of-network-service-stopped-osi-layer-2-communication | Change of Network Service Stopped OSI Layer-2 Communication | 2003 | Confirmed | Switzerland | Other | Prior to the incident, all PLCs of a control system were connected to one network service (i.e. one port of a router). Over this service, all were also able to use the OSI layer-2 (ISO) protocol for communication. Due to an exhaustion of all IP addresses in the subnet, the PLCs were regrouped into two services by connecting some of them to another port of the same router. Unfortunately, due to the fact that the ISO protocol was not bridged between the two services, the inter-communication between the PLCs of both groups ceased. | The production control system was off for some days. | The corresponding router has been re-configured to handle the ISO protocol bridging. Nevertheless, a final solution will be the termination of all non-IP protocols by the end of 2004.
 | https://www.risidata.com/Database/Detail/union-carbide-chemical-leak-west-virginia | Union Carbide Chemical Leak West Virginia | 1985 | Confirmed | United States | Chemical | The Institute facility leaked methylene chloride and aldicarb oxime, chemicals used to manufacture the pesticide Temik. The leak resulted from a computer program that was not yet programmed to recognize aldicarb oxime, compounded by human error when the operator misinterpreted the results of the program to imply the presence of methyl isocyanate (as in Bhopal). (#3) | One hundred and thirty four people were were sent to the hospital, six of whom where Union Carbide employees.   Thirty people filed two lawsuits seeking $88 million in damages, but hundreds of people marched in support of the company.   OSHA proposed fines of $32,100 for endangering workers, though later agreed to having Union Carbide pay $4,400 if it bought an accident simulator for training workers. (#1) | Union Carbide spent $5 million to improve safety systems, but two more leaks occurred in February 1990. (#1)
 | https://www.risidata.com/Database/Detail/virus-infection-of-operator-training-simulator | Virus Infection of Operator Training Simulator | 2002 | Confirmed | Canada | Petroleum | An operator training simulator was shipped to the site from the manufacturer in Houston. Prior to connection to the training DCS system, a standard system procedure check detected that the simulator was infected with a common computer virus. 	It is unlikely that this virus would have had any direct impact on process operations as the training system was not going to be  connected to the real time process systems. | None. | Contacted manufacturer to improve quality assurance program.
 | https://www.risidata.com/Database/Detail/accidental-remote-uploading-of-plc-program | Accidental Remote Uploading of PLC Program | 2000 | Confirmed | Canada | Petroleum | A testing and programming facility for the petroleum company’s PLCs was setup by an engineering consultant in a remote city. An engineer who had just returned from the plant site used his laptop to connect to a PLC he believed was in his office. Instead the programming software connected to the last connected PLC, which was a live PLC at the plant site. Not realizing this, the engineer uploaded some trial software to the live PLC causing shutdown of a critical pipeline. | The petroleum company lost approximately 1/2 day of operations and about 10,000 barrels of production. | The oil company changed management procedures so that the controls group responsible for operations had knowledge and responsibility for all remote testing and engineering of PLC systems.
 | https://www.risidata.com/Database/Detail/y2k-test-crashes-reactor-computer | Y2K Test Crashes Reactor Computer | 1999 | Confirmed | United States | Power and Utilities | The problem began just after lunch on Feb. 8, when a group of technicians tested a computer called the “Rodworth Minimizer.” The unit, which operates when the reactor is at low power, analyzes the position of “control rods” in the core and tells engineers which ones can—and cannot—be removed to balance power distribution.	To simulate Jan. 1, the technicians had intended to connect the Rodworth to another computer that would serve as a clock. But instead of connecting the unit to the external clock, a programmer inadvertently reset the date on the backup and primary operations monitoring systems, which are not yet Y2K compliant.	As soon as the date was reset, the screens in the control room went blank.	After discussion the team decided to restore the operations system in a piecemeal fashion, testing each part as they went along. They didn’t want to bring up the system only to have it crash again.	The whole task took seven hours.
 | https://www.risidata.com/Database/Detail/scada-attack-on-production-plant-of-global-chemical-company | SCADA Attack on Production Plant of Global Chemical Company | 2001 | Likely But Unconfirmed | Unknown | Chemical | A disgruntled former employee was allegedly trying to disable the plant’s conveyor control, material storage, and chemical operating systems but was caught by a programmer ‘happening to notice unusual activity.' (#1)
 | https://www.risidata.com/Database/Detail/electronic-sabotage-of-petroleum-companys-gas-processing-plant | Electronic Sabotage of Petroleum Company’s Gas Processing Plant | 2001 | Likely But Unconfirmed | United States | Petroleum | A gas processing plant operated by US petroleum company was hacked by a supplier, sabatoging the plant. (#2)	The plant’s supplier allegedly tried to cover a mistake it made on one computer system by creating a diversion by hacking into three other systems.  (#1) | This resulted in shutting off the flow of material to homes and businesses in a Western European country. This incident resulted in contract violations, environmental fines, increased expenses, and lost revenue for the petroleum company. It took investigators six months to determine what happened and to identify the perpetrator. (#1)
 | https://www.risidata.com/Database/Detail/computer-error-at-sellafield-nuclear-plant-in-uk | Computer Error at Sellafield Nuclear Plant in UK | 1991 | Likely But Unconfirmed | United Kingdom | Power and Utilities | A computer error at the vitrification plant resulted in two shielding doors being left open while highly radioactive material was still inside one chamber. (#2) | Production at the facility was stopped and did not resume until cause of the accident was established.No one was exposed to radiation during the incident. (#2) | A patch was added. (#2)
 | https://www.risidata.com/Database/Detail/olympic-pipeline-rupture-and-subsequent-fire | Olympic Pipeline Rupture and Subsequent Fire | 1999 | Confirmed | United States | Petroleum | At about 3:28 pm PDT, a 16 inch diameter steel pipeline ruptured and released about 237,000 gallons of gasoline into a creek that flowed through Whatcom Falls Park in Bellingham, WA. About 1 1/2 hours after the rupture, the gasoline ignited and burned approximately 1 1/2 miles along the creek. (#1)	A number of events led to the rupture of the pipeline. First, was excavation damage done to the pipeline, possibly between 1993 and 1994. Second, was the construction and startup of a new products terminal, where pressure relief valves that were installed, were improperly configured or adjusted. Finally, on the day of the accident, the SCADA system that controllers used to operate the pipeline became unresponsive, (possibly due to the practice of performing database development work on the system while it was being used to operate the pipeline), making it difficult for controllers to analyze pipeline conditions and make timely responses to operational problems. (#1) | Two 10 year old boys and an 18 year old young man died as a result of the accident. Eight additional injuries were documented. A single-family residence and the city’s water treatment plant were severely damaged. Fines and litigation could bring the total damages into the hundreds of millions of dollars. As of Jan 2002, total property damages were $45 million. (#1) | Computers have been upgraded. Physical and electronic security of the SCADA systems have been addressed. All dialup modems and external connections have been removed. All connections to SCADA system now have VPNs. Virus protection has been added to administration workstations. SCADA and VMS security audit logs are reviewed routinely and network vulnerability assessments are conducted quarterly. (#1)
 | https://www.risidata.com/Database/Detail/oakland-air-traffic-control-center-outage | Oakland Air-Traffic Control Center Outage | 1995 | Confirmed | United States | Transportation | One of the three power sources was down for testing and maintenance at the time of the episode. The second power source failed unexpectedly. When technicians attempted to bring the third power source on-line, a faulty circuit board in Critical Power Panel failed, preventing power from being restored. (#1) The reason for these failures appears to be examples of problems with aged computers being used for air-traffic control. (#2) | All radar and radio communications at Oakland Center were shut down as the result of a  45-minute power outage. All radar screens went dark and all radio communications were cut off. Lights and telephones were unaffected. It took 45 minutes to restore radio and the backup DARC (direct access radar channel) radar system and it was more than an hour before the NAS computerized radar was restored. (#1)	The center lost all radar and radio contact with airborne planes within an 18-million square-mile area. (#2) | Changes were made to the way critical power feeds were wired and maintenance was done. Contingency plans were rewritten and controllers received training in how to execute them. The FAA may lobby Congress for funding to provide redundant links to radio and radar sites so that when one ARTCC has a catastrophic failure, adjacent facilities can take control of the airspace. (#1)
 | https://www.risidata.com/Database/Detail/two-viruses-cause-near-miss-with-process-control-networks-pcn-in-africa | Two Viruses Cause Near Miss With Process Control Networks (PCN) in Africa | 2004 | Confirmed | Chad | Petroleum | A serious near miss with the Honeywell (All Servers) Process Control Network occurred due to viruses entering and infecting may of the servers.  The impact of this was server and communications failure through out the system from the wells and manifolds to the Floating Production Offshore Platform FPSO.  The process control system was kept functional during the entire process of identification and resolution of the problem. Two worms | Loss personnel time and productivity. Several system and process changes. | Mandatory enforcement of company cyber and work process security best practices.
 | https://www.risidata.com/Database/Detail/korean-air-line-b747-cfit-accident-in-guam | Korean Air Line B747 CFIT Accident in Guam | 1997 | Confirmed | Guam | Transportation | Approaching Won Pat International Airport at night, Korean Air Lines Flight 801 impacted Nimitz Hill at 658ft, nearly 800ft below the minimum altitude at that point on the approach.	While initially the accident seemed to have little to do with automated systems, it turned out that the Minimum Safe Altitude Warning (MSAW) System used by the Agana tower controllers and installed at nearby Andersen Air Force Base, some 10 nautical miles beyond the departure end of the runway, had unbeknownst to controllers, not been operational due to software errors in a new software installation. It was due to a bug introduced into the MSAW system by a software upgrade developed by the FAA Technical Center in New Jersey intended to reduce the number of false-positive alarms the system was generating, in response to complaints by the Cerap operators. During subsequent testing at 191 MSAW installations in the U.S., three occurrences of software errors were found and corrected. Investigators said that the lack of MSAW advisories did not cause the crash, but could have helped prevent it.	Furthermore, when the descent profile and CVR transcript became available, questions were raised about the crew’s “resource management” that are also pertinent to dealing with more recent automation and procedures. (#1)	A landing-guidance system known as the glide slope, which guides planes to the runway, had not been in service at the airport for a month said sources at the FAA. According to a notice the agency sent pilots, the system was to be down for maintenance until mid September. When glide-slope guidance is not available, pilots can use other methods, including an electronic device that gives them their distance from the airport. Knowing that distance, they follow a stair-step pattern to the runway. (#2) | There were 254 passengers and crew aboard the aircraft; 228 lost their lives.
 | https://www.risidata.com/Database/Detail/london-august-2003-power-blackout | London August 2003 Power Blackout | 2003 | Confirmed | United Kingdom | Power and Utilities | The blackout was caused by a sequence of events. During this time period a scheduled maintenance shutdown was underway on one circuit of the line. Next, an alarm was received at the Electricity nation Control Centre, indicating that a transformer or its associated shunt reactor was in distress and could fail causing signigicant safety and environmental impacts. National Control contacted EDF Energy and asked them to disconnect the distribution system from the transformer, and switch.	After the switching process took place the automatic protection equipment on a circuit interpreted the switching as a fault. The automatic protection relay disconnected this circuit from the rest of the transmission system causing a loss of supply. (#1) | 724MW of supplies were lost amounting to around 20% of total London supplies at that time. This affected about 410,000 of EDF Energy’s customers and supplies were lost to parts of London Underground and NetworkRail. (#1) | Improvements are being reviewed in communications, security of electricity supplies, automatic protection equipment, management of protection systems, as well as operating procedures. (#1)
 | https://www.risidata.com/Database/Detail/ethernet-network-storm-zaps-multiple-plc5s | Ethernet Network Storm Zaps Multiple PLC5’s | 2003 | Confirmed | United States | Pharmaceutical | The company has an Ethernet network of 42 PLC5-E (5/20, 5/40) with redundant SCADA servers. PLCs are grouped, 3-5 together on coax 10base2 trunk/drop lines. These then go thru media converters to redundant Cisco switches with redundant fiber uplinks. The SCADA network and operator terminals are also on network, but not connected to the IS network or the outside world.	An intermittently faulty fiber caused the switches to generate multiple Spanning Tree Protocol discovery messages. The number of these escalated over several hours until one switch locked up and nothing would move on the network. (the switch terminal port locked up as well). At the same time, 13 PLC’s on several different trunk lines faulted with solid red fault light. All PLC programs were wiped and the PLCs required a power cycle before even serial comms were possible.	Rockwell PLC5 Release Notes (1) state that “Series C, revision H and later processors limit the amount of messages they will accept under extremely high levels of Ethernet traffic (storms). This is designed to prevent a fault with memory loss.” The PLCs in question were Series C Revision E. Thus it is likely that this incident was due to the Ethernet traffic storm caused by the STP flapping. | Lost production for 2 _ hours and a batch in process had to be destroyed. | All PLCs were upgraded to a newer version of firmware.
 | https://www.risidata.com/Database/Detail/uk-air-traffic-control-computers-fail | UK Air Traffic Control Computers Fail | 2002 | Likely But Unconfirmed | United Kingdom | Transportation | Air-traffic control computers at the West Drayton control center, near Heathrow Airport, failed causing subsequent failures at the control center in Swanwick, Hampshire. Although the current failure is being attributed to “creaky” old systems that are unstable, the previous incident was attributed to a data-input error. (#1) | The glitch meant that all the routes and schedules information normally produced by the computer - called flight strip - had to be prepared by hand. (#2)
 | https://www.risidata.com/Database/Detail/errant-antivirus-definition-brings-down-railway-lans | Errant AntiVirus Definition Brings Down Railway LANs | 2005 | Confirmed | Japan | Transportation | An errant virus definition distributed by a anti-virus PC software company, Trend Micro, caused a large scale disruption the reservation division of railway company JR East. The software company’s automatic update site in the Philippines released a new virus definition file for its Virus Buster anti-virus software which was not adequately tested. This file was picked up by many users in Japan and abroad who either automatically or manually invoked the virus definition update function of the software. Windows XP SP2 and Windows 2003 server users with this software installed who updated their definition file AND rebooted the PC after the update (as suggested by the software) saw the CPU usage go up to 100% immediately after booting and could not control their systems.	on their PCs. (#1) | The JR railway reservation division could not check the status of the reservation system. Agents diverted all telephone customers to manned counters at railway stations. (#1) LANs became inaccessible at the  reservation division of railway company JR East. The company put the user	number around 10 million individual users. (#2)
 | https://www.risidata.com/Database/Detail/scada-workstation-infected-by-w32-korgo-worm | SCADA Workstation Infected by W32/Korgo Worm | 2004 | Confirmed | United States | Power and Utilities | The SCADA operator workstations got hit with the W32/Korgo worm virus. They were out of service starting at 14:33 and one terminal was back by 16:15. The first terminal was back by 18:23. These three terminals were on the corporate Intranet outside the SCADA firewall. | Immediate installation of a Microsoft patch on all impacted workstations to correct the problem. Then rolled out Anti-virus software and the same Microsoft patch on all other SCADA workstations inside the SCADA firewall. A new subnet and screening router for these three Intranet connected workstations was put into place. | Working to roll out the Microsoft patches into the SCADA HMIs much quicker. McAffee epo agent was installed on all SCADA workstations to update dat files daily.
 | https://www.risidata.com/Database/Detail/slammer-impacts-offshore-platforms | Slammer Impacts Offshore Platforms | 2003 | Confirmed | United States | Petroleum | The operator of the oil platforms was aware of the SQL vulnerability and was actively pushing patches out to HMIs and database servers located on the platforms. In many cases the patches had been installed but the systems had not been rebooted due to a lack of qualified IT personnel on the platform. The company was in the process of helicoptering IT support staff to each of the platforms to supervise the reboot of the systems when the Slammer outbreak occurred. | Loss of control system view and data collection.
 | https://www.risidata.com/Database/Detail/sql-slammer-impacts-drill-site | SQL Slammer Impacts Drill Site | 2003 | Confirmed | United States | Petroleum | SQL Slammer virus entered the Corporate Intranet from the internet onto the Automation Segments/VLANS. This Denial of Service attack produced large enough quantities of traffic to use up resources on routers and switches. SQL sources on the Automation segment were 3 PSs running Bently Series One software and 2 PCs with the newer version of WonderWare that uses MS-SQL server. These 5 PCs represent only a small portion of the 70+ PCs on the Automation segments and were distributed at 3 different facilities. The business segments on site also had several MS-SQL server apps running. Although separated by VLANS, the Automation and Business traffic shared common switches and routers. | Traffic was intermittent between Operator Consoles and the SCADA servers. Traffic was also intermittent between those Drill site PLCs/DeltaV DCSs and the SCADA system that were connected by Ethernet. Although the site was isolated from the corporate intranet early on, there were enough local business apps running MS-SQL server that were infected that the operators at one of the facilities did not have any remote alarming for their drill sites for several hours. At the time of this incident, the main facility DCSs had a proprietary, non Ethernet HMI interface so facility impact was minimal. Impact to support staff was significant as it took several days to track down and patch all Automation and Business offending systems. | Patched or removed the MS-SQL servers on the Automation segments and installed separate firewall, routers and switches between the corporate intranet and the Automation network.
 | https://www.risidata.com/Database/Detail/contractor-accidentally-connects-to-remote-plc | Contractor Accidentally Connects to Remote PLC | 2003 | Confirmed | United States | Chemical | A consulting engineer accidentally connected to and reprogrammed an AB PLC several states away, via the corporate VPN/WAN. The engineer had previously been working on the affected PLC at the facility earlier in the week. Due to the PLC programming software design, when the engineer returned to his office he inadvertently reconnected to the last attached PLC (the one in the plant) all along thinking he was working on a test PLC in his office.
 | https://www.risidata.com/Database/Detail/welchia-worm-infects-automation-network | Welchia Worm Infects Automation Network | 2003 | Confirmed | United States | Petroleum | Although internet access is generally denied from the Automation Network, the firewall was opened up to an Automation engineer to get an application update. In addition to getting the update, he also brought in the Welchia worm. | The worm tried to automatically connect to the internet to download a fix for the MSBlaster worm. ALthough the traffic increased on the Automation network, there was no outage because access to the internet had been closed so the requested downloads did not take place.
 | https://www.risidata.com/Database/Detail/paper-company-control-system-hit-by-blaster | Paper Company Control System Hit By Blaster | 2003 | Confirmed | United States | Pulp and Paper | Sappi Fine Paper was using Nessus and ISS Internet Scanner vulnerability testing software, as well as applying all patches to the systems but were still being hit by vulnerabilities. One machine had 40 vulnerabilities reported. The ISO knew they needed  to improve their security. When hit by Blaster worm the process control systems got infected. (#1) | They installed Core Impact on their Windows XP SP2 system, 1.5 gig memory, 256 MB memory. Now they experience less outages. (#1)
 | https://www.risidata.com/Database/Detail/trojan-backdoor-on-water-scada-system | Trojan Backdoor on Water SCADA System | 2004 | Confirmed | Canada | Water/Waste Water | During a security audit of the SCADA system, a trojan backdoor was located on a human machine interface (HMI) computer.	This trojan contained a keylogger and reverse tunnel to an outside website. It is believed that the HMI was infected by an operator browsing external hot-mail websites as the trojan was mail based.	The firewall blocked the HTTP reverse tunnel, but not the key logger which used SMTP for transport. The HMI was on the enterprise network for the regional government. Multiple Internet connections in different agencies allowed both web and email access from the HMI. | None. | Firewalls were modified to prevent HTTP access from the SCADA system computers to external websites. 	Antivirus software and procedures were invoked for all SCADA computers.
 | https://www.risidata.com/Database/Detail/plc-crash-in-food-plant | PLC Crash In Food Plant | 2005 | Confirmed | United States | Food & Beverage | Experienced an unexplained PLC-5 failure (solid red light). Complete battery lead reset was required to revive the PLC. Complete loss of DH+ and Ethernet connectivity.
 | https://www.risidata.com/Database/Detail/routine-audit-of-scada-laptop-identifies-virus | Routine Audit of SCADA Laptop Identifies Virus | 2005 | Confirmed | Australia | Water/Waste Water | A routine audit of dial in Laptop’s was conducted. Staff were reluctant to bring in laptops regularly to allow patches and upgrades. Three virus types were found and were active hence the reason for not wanting to provide the laptops. | Minimal impact due to SCADA computers on network having regular and current patches and upgrades. | Security access policies are now implemented on laptops.	Because culprit of the virus was not confirmed, all the staff were cautioned and warned of Corporate IT Policy. 	The virus intrusion on laptop was not malicious.	LAN security under audit improvement system under QA. Awaiting feedback information sharing from external sources, eg BCIT, to assist in improvements. A proposal is in process to have "dail in access" via secure VPN in DMZ on firewall for operator access to system. Also the possibility of arranging operator connection to perform automated auditing and updates to virus software and O/S.
 | https://www.risidata.com/Database/Detail/baseline-audit-uncovers-virus-in-water-control-system | Baseline Audit Uncovers Virus in Water Control System | 2003 | Confirmed | Australia | Water/Waste Water | A initial systems audit was conducted on all systems from HMI through to control electronics to establish system baseline status for each telemetry control system.	Water Control system was performing poorly and database errors were suspected. System patches were not current and virus software version was out of date. The system crashed while checking performance issues. The Blaster virus was then discovered. | Impact was minimal due to the time of the failure. | All systems are now under a patch / update management scheme. Systems now on SCADA LAN. The security of the LAN is now under an audit improvement system awaiting feedback and information sharing from BCIT to assist in improvements.
 | https://www.risidata.com/Database/Detail/single-plc-lost-for-unknown-reason | Single PLC Lost For Unknown Reason | 2003 | Likely But Unconfirmed | United States | General Manufacturing | A single PLC5 was lost while saving on-line edits using Al software. The potential culprits considered were:	1. An Ethernet broadcast storm	2. ControlNew grounding and/or message overload	3. Processor over-capacity 4. Noise on CH0. All of the above were deemed unlikely. The only way to clear the processor was to reload the program from disk. The processor is old - PLC5-40C (1.25!) with an ethernet sidecar. Had just completed on-line editing (using AI software), and went to save changes to the hard drive. About 70% of the way through the save process, the processor faulted (solid red light) and the save failed (got a message about a DH+ failure - when programming via ethernet!). Used about 85% of the available processor memory. Also, the only change to the network in the last 3 years is that the BNC connector that used to plug into the programming PC now plugs into a hub, and the PC now connects to the hub using an RJ-45 connector. Nothing else is connected to the hub.  Have since reworked a good chunk of the code and dropped processor utilization down to about 60%, distributed some of the control to other network nodes to allow reduction of about 90% of the ethernet messaging that was taking place, removed the ASCII code that wasn’t being used anyway, reduced the amount of data that the Cnet PanelView’s were polling for, and now do edits on that line only when we are in transition periods (where an ‘unplanned’ shutdown won’t hurt).
 | https://www.risidata.com/Database/Detail/runaway-remote-control-train | Runaway Remote Control Train | 2002 | Likely But Unconfirmed | United States | Transportation | A runaway train ploughed through NIPSCO’s Michigan City Generating Station hitting another locomotive before the second locomotive’s engineer narrowly jumped to safety. The train was operating with a remote-controlled system that was less than one year old. The unmanned eastbound diesel-electric engine was pushing six coal cars and travelling at about 30 m.p.h. as it approached the coal drop-off area. The train did not respond to the radio controls and smashed through the enclosed thaw shed and coal rotary dumper. The impact sent the second train about 200 feet crashing through a fence and uprooting a bumper post intended to halt runaway trains.  The train also ripped the track’s rails and dislodged the snow blade from the second train. (#1) A spokesman blamed the crash on a switch malfunction. But the system was supposedly designed so that if the remote-controlled engine receives no signal, its brakes should automatically engage. (#2) | Operations were not affected. (#1)
 | https://www.risidata.com/Database/Detail/offsite-fiber-cable-cut-causes-loss-of-communications | Offsite Fiber Cable Cut Causes Loss of Communications | 2005 | Confirmed | United States | Power and Utilities | There was a major fibre optics cut east of Russelville, Arkansas. The resident Nuclear Regulatory Commission person notified Emergency Planning that their contact with Arlington and headquarters was out, including commercial lines. Subsequent investigations revealed that ERCS, END and Health Physics Network lines were not functioning at Arkansas Nuclear One. The fibre optic cut was repaired and all phone circuits were operable by 1618 CST April 6, 2005. (#1)
 | https://www.risidata.com/Database/Detail/computer-error-grounds-japanese-flights | Computer Error Grounds Japanese Flights | 2003 | Confirmed | Japan | Transportation | Flights were cancelled and many others were delayed at airports across Japan after the nation’s main air control system broke down. The system has a backup, but both systems went down at the same time. (#1) The troubled flight data-processing system at Tokyo Air Traffic Control Center in Tokorozawa, Saitama Prefecture, automatically transmits flight information to airports across Japan.  The system had some programs partially replaced in the system that exchanges flight plans with the Defense Agency, and it went down immediately after it was turned on following the replacement. (#2) | According to airline officials, more than 20,000 passengers were affected. All aircraft for domestic and international flights were grounded for at least 30 minutes after the failure. About 1,000 flights were delayed for 30 minutes or longer with the longest delay being five hours and 50 minutes. (#1)
 | https://www.risidata.com/Database/Detail/software-bug-blamed-in-radioactive-spill | Software Bug Blamed in Radioactive Spill | 2002 | Likely But Unconfirmed | Australia | Mining | A pipe burst at the uranium mine and in about four minutes 62,000 litres of radioactive liquid escaped from the pipe into an area around the uranium processing plant. The leak was an acid solution containing a mixture of chemicals including innocuous salts, as well as radioactive and toxic chemicals. The most significant of which was uranium. There had been a computer error and human error had caused the accident. A chain of events contributed to the accident. - Loss of power to the computer system that controls and monitors plant functions. - Backup power was cut off as a result of what appeared to be failure to follow correct procedure during maintenance. (This has been challenged by Heathgate Resources, the owners of the mine). - Incorrect computer programming resulted in pumps continuing to operate after power to the control system had been lost. - Pressure switches failed to operate and shut down the pumps.  - Piping failed at well below its rated pressure. (#3)
 | https://www.risidata.com/Database/Detail/computer-flaw-makes-water-undrinkable | Computer Flaw Makes Water Undrinkable | 1998 | Likely But Unconfirmed | United States | Water/Waste Water | A computer glitch shut down the chlorination system and caused the chlorine content of the city water to drop below the safety threshold, affecting 40,000 residents. | This occurred during the night and was not discovered until a routine check 14 hours later. Notices were then sent out to 9,000 homes advising people ‘to boil the water before drinking.’ | The city has now installed an automatic system to notify an on-call supervisor in case this recurs.
 | https://www.risidata.com/Database/Detail/code-red-worm-defaces-automation-web-pages | Code Red Worm Defaces Automation Web Pages | 2001 | Confirmed | United States | Petroleum | A network monitoring tool (PC running HPOpenView) used on the business side was also being used on the Automation Networks. This computer had two NIC cards installed, one on each network. This network monitoring PC provided a path from the internet, via the company business network onto the Automation network. | Automation web pages were defaced. | Provided separate system for Network Monitoring for each Network.
 | https://www.risidata.com/Database/Detail/sasser-worm-infection-in-process-control-system | Sasser Worm Infection in Process Control System. | 2004 | Confirmed | United Kingdom | Petroleum | A worm (Sasser) infected a number of HMIs on a process control network. Initial point of infection suspected as from an external network connected to the same firewall as the PCN. None of the HMIs were installed with AV software, and none had been patched. All were running on a Windows based OS. The firewall, although running an IDS service, did not have a fully secured rulebase, which allowed the infection to pass onto the PCN. | None, as not all HMIs became infected. | AV software has now been distributed to all HMIs, and is regularly updated, as are MS patches as released.
 | https://www.risidata.com/Database/Detail/worm-attack-on-drilling-control-system | Worm attack on Drilling Control system | 2004 | Confirmed | United Kingdom | Petroleum | A new HMI type machine on a drilling network was introduced without AV software by a third party drilling company. This machine was also not fully patched. This was against standard procedures. Other machines on the same network had not received the latest patch. The HMI was also infected with a worm which caused the control system to be infected by the worm. This led to loss of view of the drilling control network; however, operations were suspended at the time of the incident for modifications to the drilling asset. Therefore, impact to operations was negligible | Negligible, as operations had already been suspended for other reasons.
 | https://www.risidata.com/Database/Detail/ping-sweep-caused-dos-on-pcn-firewall | Ping Sweep Caused DOS on PCN Firewall | 2005 | Confirmed | United Kingdom | Petroleum | An open rule-base on a PCN firewall allowed a ping sweep from an external network onto the PCN to use up all available licenses on the PCN firewall. This meant the firewall denied connectivity to genuine process related traffic into the PCN. | Manual procedures used to obtain remote data feed information into PCN. | Restrict firewall rulebase to remove open rules allowing generic ping access to large numbers of internal hosts.
 | https://www.risidata.com/Database/Detail/sasser-worm-causes-loss-of-view-in-chemicals-plant | Sasser Worm Causes Loss of View in Chemicals Plant | 2004 | Confirmed | United States | Chemical | The Sasser worm infected the DCS of a major chemicals plant. The worm infection came through the firewall on port 445 which was needed for the PHD authentication connection. The worm caused loss of view and loss of control for the whole of the DCS control system for the entire plant for over 5 hours. | Raised awareness significantly. | Raised profile of security programme and improved architecture.
 | https://www.risidata.com/Database/Detail/infected-new-hmi-infects-chemical-plant-dcs | Infected New HMI Infects Chemical Plant DCS | 2003 | Confirmed | Europe | Chemical | A vendor/contractor built a new HMI on their own premises & network and did not install all appropriate security patches. The HMI became infected with a worm (blaster?). The HMI was walked passed the PCN firewall and was connected to the PCN when it infected other machines on the DCS. | Minimal | Raised awareness. Initiated a process & guidance for ensuring that control system devices are checked for virus/worm infection prior to being connected to process control networks.
 | https://www.risidata.com/Database/Detail/mumu-infection-of-leak-detection-system | MUMU Infection of Leak Detection System | 2003 | Confirmed | United Kingdom | Petroleum | A gas pipeline leak detection system became infected by the MUMU worm owing to a weak admin password. The infected machine was removed from the network for a few days but there was no operational impact. | None. | Tightened admin passwords
 | https://www.risidata.com/Database/Detail/mumu-infection-of-fiscal-metering-system | MUMU Infection of Fiscal Metering System | 2003 | Confirmed | United Kingdom | Petroleum | A fiscal metering leak system became infected by the MUMU worm owing to a weak admin password. The infected machine was removed from the network for a few days but there was no operational impact. | None. | Tightened admin passwords.
 | https://www.risidata.com/Database/Detail/mumu-infection-of-operator-training-system | MUMU Infection of Operator Training System | 2003 | Confirmed | United States | Petroleum | The AV software detected a virus on the Operator Training Simulation system GUS stations. This training system was not behind a firewall. The worm was removed and the passwords hardened. | None. | Tightened admin passwords.
 | https://www.risidata.com/Database/Detail/blaster-worm-infects-chemical-plant | Blaster Worm Infects Chemical Plant | 2003 | Confirmed | United Kingdom | Chemical | A couple of HMIs became infected by the nachi worm. The worm entered the control system through a poorly configured firewall. The worm infection was discovered by the AV software. No production was lost. The worm was removed and the firewall rules were tightened. | None. | Tightened firewall rules.
 | https://www.risidata.com/Database/Detail/virus-worm-infects-new-oil-platform | Virus/Worm Infects New Oil Platform | 2003 | Unknown or Unlikely | Norway | Petroleum | There were strong (but unconfirmed) rumors that a new platform Grane experienced production shutdown because of a virus attack hitting the ABB control system. The Grane ABB system is based on ABB’s latest Industrial IT platform. Information is that ABB had not installed virus checking software on this system initially because of concerns regarding adverse affects on performance. ABB and the company have apparently had a major investigation and shake up in how they handle similar situations in the future and it is understood that virus checking software is now installed. | Raised concern and awareness. | Installed AV.
 | https://www.risidata.com/Database/Detail/blaster-infects-onshore-oil-production-control-system | Blaster Infects Onshore Oil Production Control System | 2003 | Confirmed | United States | Petroleum | This scada system that controls an onshore oil production system was infected by the blaster worm. This seemed to cause no impact to operations and was only noticed by IT security personnel who were trying to clear the corporate network of the worm infection. | None. | Patched machines.
 | https://www.risidata.com/Database/Detail/backdoor-trojan-attack-on-manufacturing-lab | Backdoor Trojan Attack on Manufacturing Lab | 2004 | Confirmed | Unknown | Electronic Manufacturing | This incident describes a complex and wide reaching malware-based attack against the manufacturing lab systems of a major electronics manufacturer. The lab was a large integrated test and development facility with a significant number of Windows servers and development machines spread over several building sites. The attack was a backdoor trojan that was at the time a new and unknown variant. Whether this was a directed attack and the intent of the attack is not known. Initially, it appeared that only one server had been infected and then cleaned automatically by its anti-virus software. Inspection of the anti-virus logs on this server indicated that the virus had been deleted. Unfortunately, later investigation proved this not to be the case. The virus had created a file named administrator.txt which contained a list of IP addresses for all the lab machines, along with all of the account names for each machine recorded, and the password for that account. Many of the accounts that were recorded were local administrator accounts with blank passwords or passwords consisting of the phrase “password”. The virus had configured an ftp server and was sending this information to an unknown location. The server was disconnected and the administrator.txt file was printed.  Another server was experiencing similar problems and a decision was made to disconnect this server from the network as it most likely had a virus. However, the users refused as they couldn’t spare the down time so management was then asked to disconnect the infected lab machines which would result in decreased production and therefore cost money. In a few short hours, at least half of the lab machines were discovered to be infected and were disconnected from the network (with production stoppages). From here, the issue was escalated and corporate entities were contacted to share information. The corporate network and desktop support venders were informed of the situation and a call was made to the organization’s network security. A representative at the anti-virus software vendor was also contacted. The problem was considered contained by the end of the day but not solved. Almost a week went by and there was a desperate need for an immediate solution. The engineers decided to invoke the equivalent of a mutiny by reconfiguring the test beds with the machines hooked to hubs and switches for connectivity. There was no access to DNS servers, no communication process and no documentation for changing the many embedded passwords. There was no official fix yet available and some valuable resources were not properly backed up. Ultimately, users were helped with work-arounds until the network and all related resources were up and running. | All in all about 3 weeks of development time and countless other related hours were lost although the actual number is not known. | Significant procedural changes were made to minimize the chance and impact of a similar incident.
 | https://www.risidata.com/Database/Detail/11-ethernet-plcs-fail-at-once | 11 Ethernet PLCs Fail At Once | 2003 | Likely But Unconfirmed | United States | Metals | Around 11pm every PLC5 Ethernet controller in our facility lost its memory (11). Processors that were only on DH+ were fine. Furthermore, our GE PLC’s on the network were unaffected. To fix the problem we configured the Ethernet port via the serial port and then we downloaded new programs via Ethernet. All the batteries are good. Our DH+ PLCs were not affected as well as SLC 505 E’s. The GE PLCs are Ethernet and again did not have a problem. So our problem seems to be with PLC 5/X0 E. We do have a mix of 80E, 40E and 20E processors. We use 2 subnets one for the melt shop and one for the rolling mill. Two nights in row we lost ALL PLC 5 processors that are on Ethernet.
 | https://www.risidata.com/Database/Detail/ethernet-storm-wipes-plcs-processor-memory | Ethernet Storm Wipes PLC’s Processor Memory | 2000 | Likely But Unconfirmed | Australia | Unknown | The memory of most of our PLC5 processors was wiped and we found that it was related to the revision level of the processors. The blame was pointed at an Ethernet storm that the earlier rev. processors could not handle and wiped their memory. The Ethernet storm caused by a third party software package sending ethernet broadcast frames. We had our affected processors upgraded to Series E rev E.2 or Series D rev F.2 as well as upgrading the ethernet sidecars and no trouble since.
 | https://www.risidata.com/Database/Detail/malformed-packet-causes-plc-crash | Malformed Packet Causes PLC Crash | 2002 | Likely But Unconfirmed | United States | Unknown | There was some new security software that sent a malformed “ICMP redirect” packet to all the TCP/IP devices in the system. Older PLC-5E controllers have a firmware weakness that can make them fault if they receive this kind of packet from a misconfigured router or a security scanner.
 | https://www.risidata.com/Database/Detail/e-tag-incident | E-Tag Incident | 2005 | Confirmed | United States | Power and Utilities | An individual logs into the electronic tagging (e-tag)  system associated with a Purchasing-Selling Entity (PSE).  They had legitimate access to a valid user name, password, and digital certificate of an active account, but this login was unauthorized and was made from an off-site computer not under the direct control of the PSE.  The motive was to play a joke on the PSE scheduler. Because the account’s credentials were valid, the e-tag system accepted the login.  It is standard operating procedure for the PSE to share user accounts amongst appropriate staff within the organization.  At the time, the PSE had approximately 7-8 user accounts with over a dozen personnel all sharing access to these user accounts.  The IP address used for this access is not associated with the PSE’s network.  For business continuity reasons, the PSE allows remote access to the e-tag system from multiple locations, including remote off-site locations.  Because the user account, password, and digital certificate were valid, the e-tag vendor appropriately accepted the login. Three minutes after logging in, the person logged into this account submits an e-tag without the knowledge of the PSE.  The e-tag requests a large amount of power to be delivered on behalf of the PSE to another large control area for the next hour.  This tag was rejected by the generation control area’s automated e-tag processing software.  While there were several reasons to reject the e-tag (i.e., it was submitted less than 30 minutes before the time of the request, etc.), their software cites an incorrect open-access same-time information system (OASIS) number as the reason for rejecting the transaction. During the same timeframe, the e-tag was approved by the receiving control area, and denied by an intermediate control area in the scheduling path, and then denied by the receiving control area (reversing their previous approval action).  Note that denial by any party on the e-tag will cause it to be rejected. All of the approval actions on this e-tag were complete 4 minutes after the tag was submitted.  The rejected e-tag (called a “dead” tag) is returned and posted to the PSE’s trading system, but because that screen was minimized at the time, neither of the two traders on duty at the PSE are aware that anything out of the ordinary has happened. A short while later, the person logged into this account submits a second e-tag, also without the knowledge of the PSE.  This e-tag is directed only to one control area (both e-tags specify the same load control area; the previous e-tag specified a different generation control area).  There are other similarities between this e-tag and the one submitted 6 minutes earlier, including the same power amount, the same timeframe, and similar path information.  The primary difference between this e-tag and the previously-submitted e-tag, in addition to the generating control area, are changes to the path information (thereby removing the reference to the incorrect OASIS information). Almost immediately after receiving this e-tag, control area personnel called the PSE inquiring about this most unusual e-tag that they had received from them (most notably the very large MW requested).  At this time, one of the traders on duty at the PSE looked at the e-tag vendor’s screen at his terminal, saw the e-tag that had been entered on their behalf, and immediately realized that neither he nor the other trader on duty had issued this e-tag.  He immediately withdrew the e-tag. The PSE staff then went into “high alert”.  The trader on duty at the PSE knew that he could supersede the privileges of the person logged into the account by logging into the same account himself.  (The system only allows one active session for each account, and disables any existing sessions when somebody new logs into that account.)  The trader also notified relevant control areas and notified internal IT support and other personnel within the PSE. The PSE | Loss of staff time. | The following day, the digital certificate associated with the account was revoked by the PSE's security officer.
 | https://www.risidata.com/Database/Detail/unauthorized-connection-permits-sasser-infection | Unauthorized Connection Permits Sasser Infection | 2004 | Confirmed | Unknown | Unknown | An unauthorised connection into the organisation allowed the Sasser worm to contaminate a local process control network before the relevant Microsoft Patch had been made available. | Visibility of the plant process was lost for a period. | Unknown.
 | https://www.risidata.com/Database/Detail/korgo-worm-infects-20-workstations | Korgo Worm Infects 20 Workstations | 2004 | Confirmed | Unknown | Unknown | The Korgo worm impacted 20 workstations (view only terminals) as the result of a corporate patch distribution problem. | Unknown. | Unknown.
 | https://www.risidata.com/Database/Detail/dcs-console-reprogramming-causes-gateway-fault | DCS Console Reprogramming Causes Gateway Fault | 1998 | Confirmed | Canada | Pulp and Paper | The paper mill had just completed an upgrade of its paper machine, during which a number of engineers had been brought in from head office to assist with DCS commissioning. Everyone on the DCS commissioning team knew the passwords for the control system computers and when the project was completed, no one bothered to change them. Trouble started about a month later when one of the head-office engineers decided he needed a good data source for an expert-systems experiment he was running. Using the company’s wide area network (WAN), he was able to connect into the mill network from the corporate headquarters several hundred miles away. Once into the mill’s business LAN, he was able to connect to the DCS through a link originally set-up to allow mill supervisors to view operators screens from their offices. He then loaded a small program onto one of the DCS graphics stations (a UNIX machine). This program asked all DCS devices to dump their data back to him once every five minutes. Unfortunately the engineer’s new task would occasionally overload one of the DCS to PLC communications gateways, and the DCS would stop reading the PLC data. This, of course, caused the machine operators concern as they lost control of the motors controlled by the PLCs. Soon the electrical department was busy troubleshooting the PLCs.  Eventually the problem was solved by a mill engineer who noticed that the problems always occurred at intervals that were at multiple of five minutes. Suspecting that it might be software induced, he started to inspect all the tasks running on the DCS computers and found the offending task. Of course, by then the lost production in the mill had been substantial. | Installed VLAN system and Firewalls between business and process networks
 | https://www.risidata.com/Database/Detail/maroochy-shire-sewage-spill | Maroochy Shire Sewage Spill | 2000 | Confirmed | Australia | Water/Waste Water | In November 2001, 49-year-old Vitek Boden was sentenced to two years in prison for using stolen wireless radio, SCADA controller and control software to release up to one million litres of sewage into the river and coastal waters of Maroochydore in Queensland, Australia. Boden, who had been a consultant on the water project, conducted the attacks in early 2000 after he was refused a full-time job with the Maroochy Shire government.  The Crown case on the computer hacking offences was that between 9 February 2000 and 23 April 2000 Vitek accessed computers controlling the Maroochy Shire Council’s sewerage system, altering electronic data in respect of particular sewerage pumping stations and causing malfunctions in their operations. The evidence in the case revealed that the Council’s sewerage system had about 150 stations pumping sewerage to treatment plants. Each pumping station had installed a Hunter Watertech PDS Compact 500 computer (RTU) capable of receiving instructions from a central control centre, transmitting alarm signals and other data to the central computer and providing messages to stop and start the pumps at the pumping station. Communications between pumping stations and between a pumping station and the central computer were by means of a private two-way radio system.   Vitek, an engineer, had been employed by Hunter Watertech as its site supervisor on the SCADA installation project for about two years until resigning December 3, 1999. At about the time of his resignation he approached the Council seeking employment. He was told to enquire again at a later date. He made another approach to the Council for employment in January 2000 and was told that he would not be employed. The sewerage system then experienced a spate of faults. Pumps were not running when they should have been, alarms were not reporting to the central computer and there was a loss of communication between the central computer and various pumping stations. On 16 March 2000, when malfunction occurred in the system, Mr. Yager (a Hunter Watertech investigating the problems) communicated over the network with a bogus pump station 14 which was sending messages to corrupt the system. He was temporarily successful in altering his programm to exclude the bogus messages but then had his computer shut out of the network for a short period. The intruder was now using PDS identification number 1 to send messages.  Further problems then occurred as a result of a person gaining computer access to the system and altering data so that whatever function should have occurred at affected pumping stations did not occur or occurred in a different way. The central computer (SCADA master) was unable to exercise proper control and, at great inconvenience and expense, technicians had to be mobilised throughout the system to correct faults at affected pumping stations. On the occasion the subject of count 45, a pumping station overflowed causing raw sewerage to escape. On 23 April 2000 an intruder, by means of electronic messages, disabled alarms at four pumping stations using the identification of pumping station 4. The intrusions began just after 7:30 pm and concluded just after 9:00 pm. By this time the appellant had fallen under suspicion and was under surveillance. A vehicle driven by him was located by police officers and when the vehicle was pulled over and searched, a PDS Compact 500 computer, later identified in evidence as the property of Hunter Watertech, was found in it as was a laptop computer. | Along with 27 counts of using a restricted computer to cause detriment or damage, Vitek was also convicted of 1 count of wilfully and unlawfully causing serious environmental harm.  The sewerage spill was significant. It polluted over 500 metres of open drain in a residential area and flowed into a tidal canal. Cleaning up the spill and its effects took days and required the deployment of considerable resources. “Marine life died, the creek water turned black and the stench was unbearable for residents,” said Janelle Bryant, investigations manager for the Australian Environmental Protection Agency.
 | https://www.risidata.com/Database/Detail/plcs-crashed-by-it-audit | PLCs Crashed by IT Audit | 1995 | Confirmed | United States | Food & Beverage | A security consultant was scanning the food companies business and process networks for vulnerabilities. Probe packets containing deliberately malformed entered the Ethernet-based process control network and caused all PLCs to hard fault. The packets contained malformed ICMP Redirects messages with a subcode of 4 or greater. | The loss of production was estimated to be over $1,000,000 USD
 | https://www.risidata.com/Database/Detail/hackers-gain-unauthorized-access-to-a-modular-hybrid-controller | Hackers gain unauthorized access to a modular hybrid controller | 2002 | Likely But Unconfirmed | Unknown | Unknown | A hacker or group of hackers gained unauthorized access to a modular hybrid controller resulting in a denial of service and loss of equipment control. There were two phases to the attack. First, hackers opened connections, sent unknown messages, and left without closing the connection. After repeated attacks, all connections were consumed resulting in a denial of service to legitimate users on the Ethernet port. Second, hackers sent a Web page to the controller containing Java script and the text: “Hello! Welcome to http://worm.com Hacked by Chinese.” This exposed a bug in the TCP/IP stack causing the controller to reset, forcing all outputs to their off state. | Two incidents: first caused a denial of service and loss of equipment control the second caused the controller to reset, forcing all outputs to their off state.  It is unknown what impact this had on the process being controlled by the “modular hybrid controller”. | Two controller vendor engineers worked full-time on the problem for three to four weeks each. Network activity was captured with a network analyzer. Once the causes were identified, the fixes were relatively easy. First, the controller’s software was modified to properly close all timeout connections. Second, the vendor of the TCP/IP stack software used in the controller was informed and provided a fix for the stack.
 | https://www.risidata.com/Database/Detail/malware-shuts-down-milling-factory | Malware Shuts Down Milling Factory | 2010 | Confirmed | Iran | General Manufacturing | A 350-ton capacity multi-purpose mill probably for wheat flour, was shut down due to the Stuxnet virus. | A 350-ton capacity multi-purpose mill probably for wheat flour, was shut down due to the Stuxnet virus.
 | https://www.risidata.com/Database/Detail/water-utility-hack-destroys-pump | Water Utility Hack Destroys Pump | 2011 | Confirmed | United States | Water/Waste Water | The control system of the city water utility in Springfield, Illinois was hacked.  Hackers gained remote access  to the control system causing the system to turn on and off repeatedly leading to the burnout of a water pump.  On November 8, 2011 ,  a water district employee detected the problems in the SCADA system.  The “Public Water District Cyber Intrusion” report was released by the Illinois Statewide Terrorism and Intelligence Center indicated that forensic evidence suggests that hackers may have accessed the system as early as September 2011.   According the report, The cyber attack was launched from an IP address in Russia and gained access by first hacking into the network of a software vendor that makes the SCADA system used by the utility.  Usernames and passwords were stolen from a control system vendor by the hackers and used them to access the water utilities network.   Joe Weiss, a managing partner for Applied Control Solutions, said that he obtained the report on the condition that the water utility and its location were not disclosed.  The US Department of Homeland Security released a statement indicating that the utility was located in Springfield, Illinois. Weiss published some details of the hack to bring attention to the incident reporting concerns that the ability of the US government to secure critical infrastructure.  Federal officials say they are investigating to determine if a cyber attack was responsible for the failure of a water pump failure and disputed the statements made by Joe Weiss.  Update:  Federal Officials believe there was no cyber attack on the water utility in Illinois saying the initial report from the Illinois counterterrorism center was wrong.  Homeland Security spokesman Chris Ortman said the Illinois report was inaccurate.  “There is no evidence to support claims made in the initial reports-which were based on raw, unconfirmed data and subsequently leaked to the media,”  Mr. Ortman told the Washington Times.   He also said a special team of federal investigators concluded “that there was no malicious traffic from Russia or any foreign entities” in the logs they examined from the utility.  A Homeland Security official confirmed that the Russian IP address had been discovered in the utility’s  computer system records because “a contractor in Russia on personal travel had logged on” from there to do work on it.  The director of the Illinois State Police has launched an inquiry into how the initial report was written and why it was circulated, spokeswoman Monique Bond said. | The SCADA system of a water utility was hacked.  As a result, the system turned on and off repeadedly leading to the burnout of a water pump.
 | https://www.risidata.com/Database/Detail/auto-manufacturer-hacked | Auto Manufacturer Hacked | 2012 | Confirmed | United States | Automotive | A computer virus was detected on the network of an auto manufacturer.  Unknown attackers stole employees’ IDs and encrypted passwords after planting a computer virus on the company’s computer systems.  The company waited a week to disclose the attack to allow time to investigate. The company used their own security experts in addition to a third-party security consultant to investigate the attack.  The company is still unsure where the attack came from.  The company suspects that the hackers were attempting to steal intellectual property pertaining to the company’s hybrid and electric vehicle drivetrains. | A virus was planted on the network of a major auto manufacturer.  Employee user IDs and passwords were transmitted. The company suspects that the hackers were attempting to steal intellectual property.
 | https://www.risidata.com/Database/Detail/control-system-infected-with-sqlslammer-worm | Control System Infected with SQLslammer Worm | 2003 | Likely But Unconfirmed | Unknown | Petroleum | A corporate laptop user installed software, unaware that it included an unpatched version of msSQL. Sometime later the user connected PC to the Internet (in violation of company policy) to access email via an ISP. SQLslammer infected the internet connected machine. The user then brought the infected machine into the office and connected to the network, causing a small outbreak of the SQLslammer worm within the corporate network. An unfirewalled data acquisition server, a control system and a development control system became infected with SQLslammer worm and had to be removed from the network to prevent further infection. | There was no significant impact to production, but some history data was lost during server down-time. Data had to be manually created. | The company has a significant programme to raise awareness of digital security issues and implement security improvements. The servers have been patched and a firewall is being recommended for installation.
 | https://www.risidata.com/Database/Detail/spybot-infection | Spybot Infection | 2004 | Likely But Unconfirmed | Unknown | Unknown | Malicious Code was found on a PLC. The code was date-triggered. | Equipment Trips.
 | https://www.risidata.com/Database/Detail/date-triggered-code-found-on-plc | Date Triggered Code Found on PLC | 2003 | Confirmed | Unknown | Unknown | Malicious Code was found on a PLC. The code was date-triggered. | Equipment Trips. | Unknown.
 | https://www.risidata.com/Database/Detail/omega-engineering-sabotage | Omega Engineering Sabotage | 1996 | Confirmed | United States | Electronic Manufacturing | The morning of July 31, 1996, the first worker in the door at Omega Engineering’s manufacturing plant in Bridgeport, N.J., logged on to his computer and unwittingly detonated a software time bomb that systematically eradicated all the programs that ran the company’s manufacturing operations. Ralph Michel, Omega’s chief financial officer, testified that the software bomb destroyed all the programs and code generators that allowed the company to manufacture 25,000 different products and to customize those basic products into as many as 500,000 different designs. 	Shortly before the attack, Tim Lloyd, a 37-year-old network administrator, moved the programs off individual workstations and onto Omega’s central NetWare file server. And there were no back-up tapes to access because Lloyd brought them home and reformatted them, according to testimony at his trial for computer sabotage, which ended last week in a guilty verdict.  He was sentenced to 41 months in federal prison and ordered to pay more than $2 million in restitution. | Omega suffered $12 million in damages and lost its competitive footing in the high-tech instrument and measurement market. Eighty workers lost their jobs as a result. “We will never recover,” said plant manager Jim Ferguson.  After the system went down, in desperation, the company continued to run the machines with the programs already loaded on, until they ran out of raw materials or began choking on the inventory. Michel said the company spent $2 million to reprogram the machines and lost an estimated $10 million in sales and other costs.
 | https://www.risidata.com/Database/Detail/virus-shuts-down-ac-jazz-airline-flight-planning-computer | Virus Shuts Down AC Jazz Airline Flight Planning Computer | 2003 | Confirmed | Canada | Transportation | A virus apparently attacked an Air Canada Jazz flight-planning computer at an operations centre in Halifax that provides essential information on fuelling, weather, and other variables. Without the computer’s flight information releases, aircraft cannot take off.   The problem began at about 3 p.m. EST in a flight-planning computer that provides essential information on fuelling, weather and other variables for the roughly 700 flights that Jazz operates daily between 72 centres, spokesman Dennis Erickson said from Calgary. | The problem affected only Air Canada’s regional operations.  Without the computer’s flight releases, aircraft could not take off, AC Spokesman Dennis Erickson said - adding that with the computer malfunctioning it was not clear even how many flights had been disrupted, He estimated that about 200 had been affected, some cancelled and some delayed.
 | https://www.risidata.com/Database/Detail/nacchi-virus-infects-air-canada-check-in-system | Nacchi Virus Infects Air Canada Check-In System | 2003 | Confirmed | Canada | Transportation | MONTREAL (Reuters) - The Nacchi virus infected Air Canada reservations systems, prompting the airline to warn its passengers of delays and cancellations to flights. | The virus crippled operations at Air Canada’s reservation counters at airports across the country, as well as its call centers. The virus had not attacked computer systems handling flight operations,  Air Canada spokeswoman Isabelle Arthur said.
 | https://www.risidata.com/Database/Detail/ski-gondola-worker-shutdown-control-system | Ski Gondola Worker Shutdown Control System | 2002 | Confirmed | United States | Transportation | A 19-year-old woman faces felony charges for allegedly tampering with a public gondola system, causing a series of 33 shutdowns on the 2.5-mile line that shuttles thousands of people a day over a mountaintop at this ski resort.  Alisha Sult, an operator on the gondola system, was arrested New Year’s Day on three counts of endangering public transportation. Sult is suspected of allegedly tampering with a computer system that controls the gondola. | Police were asked to investigate after a series of 32 shutdowns of the system over the three days, Mahoney said. Most were shorter than a minute. The shutdowns occurred December 26-28 during the Telluride Ski Resort’s busiest week of the winter season.  There were no evacuations from the gondola cars and no passengers were endangered, officials said Sunday. Snowmobiles were used to carry 40 people down from a mountaintop restaurant served by the line during the longest shutdown—1 hour and 20 minutes—on December 27.
 | https://www.risidata.com/Database/Detail/slammer-impact-on-ohio-nuclear-plant | Slammer Impact on Ohio Nuclear Plant | 2003 | Confirmed | United States | Power and Utilities | On January 25, 2003, Davis-Besse nuclear power plant was infected with the MS SQL Server 2000 worm. The infection caused data overload in the site network, resulting in the inability of the computers to communicate with each other.  First Energy Nuclear (the licensee’s) corporate network, which is linked with Davis-Besse’s plant network, is connected to external networks via a firewall. A firewall is a system or systems that enforce an access control policy between networks. Among the many access control policies that Davis-Besse's corporate firewall enforced was the policy of disallowing any data using the UDP into the network by closing port 1434 (MSSQL) of the firewall. This policy would have protected Davis-Besse’s networks from the MS SQL worm infection except that the corporate network had a T1 connection behind the firewall that provided a path for the worm to enter the system. This T1 line was used by one of the licensee’s consultants who provided an application software that ran on a server. This connection bypassed all the access control policies that the corporate firewall was enforcing, including the policy of preventing data that used the UDP from coming into the corporate network. The consultant’s company network server allowed use of the UDP for data transfers and was infected by the MS SQL worm. When the consultant established a T1 line connection at the licensee’s corporate site, this action opened a path by which the worm that infected the consultant’s company server was sent to the licensee’s corporate network through the T1 line. The worm then randomly infected any servers on the corporate network that had port 1434 open. Because the MS SQL worm resided in only memory, shutting down the server removed the worm from the server’s memory, ridding the server of the infection. The licensee isolated the server from the site network, installed the MS security patch, and reconnected the server to the site network. | The slowness in computer processing speed began in the morning and by 4:50 p.m., the Safety Parameter Display System (SPDS) became unavailable and remained unavailable for 4 hours 50 minutes. By 5:13 p.m., the plant process computer was lost and remained unavailable for 6 hrs and 9 minutes.  Although the operators were burdened by these losses, the event was not deemed significant since the plant control and protection functions were not affected. | In response to this event, Davis-Besse implemented the following corrective actions: (1) required network services to document all external connections to internal network, (2) installed the security patch for the MS SQL Server 2000 vulnerability, (3) installed a firewall between the plant network and the corporate network, (4) established a requirement to monitor and filter the data coming into the plant network to the same standard as the corporate firewall, and (5) implemented a process for computer engineering personnel to review security patches for systems supported and install them within an acceptable timeframe.
 | https://www.risidata.com/Database/Detail/sobig-virus-strikes-csx-train-signalling-system | Sobig Virus Strikes CSX Train Signalling System | 2003 | Confirmed | United States | Transportation | The Sobig computer virus was blamed for shutting down train signaling systems throughout the East coast of the US. The virus infected the computer system at CSX Corp.‘s Jacksonville, Fla., headquarters, shutting down signaling, dispatching and other systems at about 1:15 a.m. EDT, CSX spokesman Adam Hollingsworth said (#2). | The Sobig infection resulted in a slowdown of major applications, including dispatching and signal systems. As a result, passenger and freight train traffic was halted immediately, including the morning commuter train service in the metropolitan Washington, D.C., area. The virus disrupted the CSXT telecommunications network upon which certain systems rely, including signal, dispatching and other operating systems. (#1) According to Amtrak spokesman Dan Stessel, ten Amtrak trains were affected in the morning. Trains between Pittsburgh and Florence, S.C. were halted because of dark signals and one regional Amtrak train from Richmond, Va., to Washington and New York was delayed for more than two hours. Long-distance trains were delayed between four and six hours. There were some residual delays in Amtrak service from Washington to Richmond, Va., and points south,  ``Most of the delays are in the 15 to 30-minute range on trains that travel through CSX territory,’’ Stessel said. (#2) | "CSX will work to protect its computer systems from further intrusion", Hollingsworth said. (#2)
 | https://www.risidata.com/Database/Detail/power-industry-slammer-2 | Power Industry Slammer #2 | 2003 | Confirmed | United States | Power and Utilities | The SCADA control network used frame relay. The telecommunications frame relay provider utilized Asynchronous Transfer Mode (ATM) through the telecommunications network backbone for a variety of services. The ATM bandwidth became overwhelmed by the worm, blocking SCADA traffic on the Frame Relay service. | Loss of SCADA communications.
 | https://www.risidata.com/Database/Detail/power-industry-slammer-1 | Power Industry Slammer #1 | 2003 | Confirmed | United States | Power and Utilities | A server on the utility’s control center LAN running SQL was not patched. The worm apparently migrated through the corporate networks until it finally reached the critical SCADA network via a remote computer through a VPN connection. | The worm propagated on the control center LAN, blocking SCADA traffic.
 | https://www.risidata.com/Database/Detail/worcester-air-traffic-communications-system-hack | Worcester Air Traffic Communications System Hack | 1997 | Confirmed | United States | Transportation | On March 10, 1997, an unidentified juvenile computer hacker broke into a Bell Atlantic control system used for the air traffic communications at the Worcester, Massachusetts airport, causing a system crash that disabled the phone system at the airport for six hours.  The criminal charges contained in Federal Information allege that the computer hacker temporarily disabled Next Generation Digital Loop Carrier NGDLC systems operated by NYNEX (later purchased by Bell Atlantic Telephone Company) at the Worcester Airport and in the community of Rutland, Massachusetts. NGDLC systems are programmable remote controllers used to integrate voice and data communications originating on a large number of standard, copper-wire telephone lines for transmission over a single fiber-optic cable. The Information alleges that the loop carrier systems operated by the telephone company were accessible from a personal computer’s modem. This accessibility was maintained so that telephone company technicians could change and repair the service provided to customers by these loop carrier systems quickly and efficiently from remote computers.  The hacker identified the telephone numbers of the modems connected to the loop carrier systems operated by the telephone company providing service to the Worcester Airport and the community of Rutland, Massachusetts. On March 10, 1997 he accessed and disabled both in sequence. | The crash of the NGDLC system knocked out phone service at the control tower, airport security, the airport fire department, the weather service, and carriers that use the airport. Also, the tower’s main radio transmitter and another transmitter that activates runway lights were shut down, as well as a printer that controllers use to monitor flight progress. The hacking also knocked out phone service to 600 homes in the nearby town of Rutland.
 | https://www.risidata.com/Database/Detail/salt-river-project-hack | Salt River Project Hack | 1994 | Confirmed | United States | Power and Utilities | Between July 8th and August 31st, 1994, the perpetrator, Lane Jarret Davis, accessed a computer or computers belonging to the Salt River Project via a dial-up modem on a backup computer. He was able to access data and delete files on systems responsible for the monitoring and delivery of water and power to SRP customers, as well as customer, financial and personnel records (#1). | The impacts reported on this incident are very contradictory. According to probation records (#1) Davis was able to access the canal control SCADA system for at least 5 hours, as well as accessing customer, financial and personnel records. SRP estimated that they suffered a $40,000 loss, not including the loss of productivity. The press reports (#3) and statement by Assistant Attorney General Michael Chertoff (#2) that Davis had control of the SCADA system controlling the Roosevelt Dam spill gates are believed to be incorrect. According to emails from SRP representatives to the Washington Post, the canal SCADA system and dam SCADA systems are not connected (#4).
 | https://www.risidata.com/Database/Detail/signal_problems_cause_train_delays | Signal problems cause train delays | 2013 | Confirmed | United States | Transportation | A computer glitch affected a control center in Omaha, NE., stopping some of the trains.  Trains on the Union Pacific Northwest, West,  North and Milwaukee District North lines were affected.  Delays were reported to be around a half hour on some lines. | A computer glitch affected a control center in Omaha, NE., stopping some of the trains.  Delays of a half hour were reported for some lines.
 | https://www.risidata.com/Database/Detail/virus_shuts_down_county_highway_department_network | Virus shuts down county highway department network | 2013 | Confirmed | United States | Transportation | The entire computer network of the Cook County Department of Highway and Transportation was shut down for nearly two weeks due to a virus infection.  The Department of Highway and Transportation maintains hundreds of miles of roads in the Chicago area.   It is believed that someone, possibly a county employee may have allowed a virus into their systems by surfing the internet or using a flash drive from home.   The virus caused  files to be renamed.  Ricardo Lafosse, Cook County’s  Chief Information Security Officer said the department was forced to immediately shut down the entire network until it cook be cleaned up.    The virus attacked about 200 computers.  It took five technicians working full time for 216 hours to clean up the network. | The virus attack affected about 200 computers.  It took 5 full time technicians 9 days to get the network cleaned up.
 | https://www.risidata.com/Database/Detail/shamoon_virus_knocks_out_computers_at_qatari_gas_firm_rasgas | Shamoon virus knocks out computers at Qatari gas firm RasGas | 2012 | Confirmed | Qatar | Petroleum | RasGas, the second largest producer of liquified natural gas in the world, was attacked by the Shamoon virus about 2 weeks after a similar attack on Saudi Aramco.  The company website and corporate network were shutdown by the virus for several days.  According to a company spokesman the virus did not impact production operations or cargo deliveries.  RasGas is a joint venture between Qatar Petroleum and ExxonMobil. | The company website and corporate network were shutdown by the virus for several days.
 | https://www.risidata.com/Database/Detail/process_control_network_infected_with_a_virus | Process Control Network Infected with a Virus | 2012 | Confirmed | Petroleum | The process control network (PCN) was infected with a virus.  Twelve PC’s on the process control network were infected. The virus infection did not affect production, or cause any environmental or property damage.  The virus was likely introduced via an infected laptop.   The laptop was not scanned for viruses using up to date anti-virus software. | Twelve PC’s on the process control network were infected with a virus. The virus infection did not affect production, or cause any environmental or property damage.  The virus was likely introduced via an infected laptop.
 | https://www.risidata.com/Database/Detail/gas_company_virus_infection | Gas Company Virus Infection | 2012 | Confirmed | Petroleum | An error message was found on a local operator panel.  After an unsuccessful reboot, antivirus software was installed.  An investigation revealed that the Local operator panel was potentially infected with a virus via a USB flash drive. | A gas company computer was infected with a virus via a USB flash drive.
 | https://www.risidata.com/Database/Detail/computer_virus_strikes_two_scottish_hospitals | Computer Virus Strikes Two Scottish Hospitals | 2009 | Confirmed | United Kingdom | Other | A PC worm infected the networking systems of two hospitals in Scotland.  The infection started in the laboratory computers at the Stobhill and Gartnavel general hospitals and the Beatson West of Scotland Cancer Care Centre. The systems were completely non-functional.  Appointments for cancer patients had to be postponed. Systems were shut down for 2 days. | A PC worm infected the networking systems of two hospitals in Scotland.  The systems were completely non-functional.  Appointments for cancer patients had to be postponed. Systems were shut down for 2 days.
 | https://www.risidata.com/Database/Detail/computer_glitch_causes_roller_coaster_malfunction | Computer Glitch Causes Roller Coaster Malfunction | 2012 | Confirmed | United States | Other | A computer glitch caused the Superman:  Ultimate Flight roller-coaster to become stuck at the top of a 150 foot hill. Twelve passengers were stranded for 90 minutes.  The cause of the malfunction is unknown. | A computer glitch caused the Superman:  Ultimate Flight rollercoaster to become stuck at the top of a 150 foot hill. Twelve passengers were stranded for 90 minutes.
 | https://www.risidata.com/Database/Detail/cascade_of_computer_crashes_causes_metro_system_shutdown | Cascade of Computer Crashes Causes Metro System Shutdown | 2012 | Confirmed | United States | Transportation | A computer problem caused the shut down of the Montreal metro system for about an hour.  The problem started when a computer failure caused a shutdown  on the orange line.  A cascade of computer crashes followed.  The entire computer system required rebooting. | A cascade of computer crashes caused the shutdown of the metro system.  The system was down for about an hour.
 | https://www.risidata.com/Database/Detail/computer_malfunction_blamed_for_major_sewage_spill | Computer Malfunction Blamed for Major Sewage Spill | 2012 | Confirmed | United States | Water/Waste Water | A major sewage spill occurred sending 2 million gallons of raw sewage into the Tijuana River.  A programmable logic controller failed shutting down pumps and controls.  This resulted in a sewage back up in the facility which ultimately made its way to the Tijuana River.  According to the spokeswoman for the boundary commission, Sally Spener, “if the software fails, you can handle if manually if the operator is responding appropriately”.   The operator is thought to be responsible for letting the problem get out of hand.  Mark McPherson, water quality chief for the county, said the system failure had a minimum effect on the heavily polluted river.  However, a Sea Grant biologist has reported finding antibiotic resistant bacteria in the sediment of  three urban estuaries —the Ballona Creek Estuary in Los Angeles County, Famosa Slough in San Diego and Tijuana River Estuary in Imperial Beach.  Recent upgrades totalling $92 million has not reduced pollution below legal limits. | A major sewage spill sent 2 million  gallons of raw sewage into the Tijuana River in San Ysidro, California.  A programmable logic controller failed shutting down pumps and controls.  This resulted in a sewage back up in the facility which ultimately made its way to the Tijuana River. A Sea Grant biologist has reported finding antibiotic resistant bacteria in the sediment of  three urban estuaries including Tijuana River Estuary in Imperial Beach.
 | https://www.risidata.com/Database/Detail/software_manufacturing_company_firewall_breach | Software Manufacturing Company Firewall Breach | 2012 | Confirmed | Canada | General Manufacturing | Telvent Canada, Ltd sent a letter to its customers to inform them of the investigation into a sophisticated cyber attack spanning the U.S., Canada and Spain.  The letter explains that Telvent discovered a breach of its internal firewall and security systems on September 10, 2012.  Attackers installed malware and stole project files related to OASyS SCADA.  Telvent, provides software and services that are used to remotely administer and monitor large sections of the energy industry.  Experts say that the digital fingerprints point to a Chinese hacking group known as the “Comment Group”. | Attackers installed malware and stole project files related to OASyS SCADA.  An investigation is continuing.
 | https://www.risidata.com/Database/Detail/u._s._electric_utility_virus_infection | U. S. Electric Utility Virus Infection | 2012 | Confirmed | United States | Power and Utilities | A virus infection was discovered in a turbine control system at a U. S. power plant.  The infection ultimately impacted approximately 10 computers on the control system network.   A third-party technician used a USB drive that was infected with a variant of the Mariposa virus.  The infection was responsible for downtime for the impacted systems and delayed the plant restart by approximately 3 weeks. | A virus infection was discovered in a turbine control system at a U. S. power plant.
 | https://www.risidata.com/Database/Detail/u._s._power_plant_infected_with_malware | U. S. Power Plant Infected With Malware | 2012 | Confirmed | United States | Power and Utilities | Malware was discovered at a U. S. powerplant when an employee was experiencing issues with his USB drive.  The USB drive was routinely used for backing up control systems configurations within the control system environment.  The employee asked an IT person to check the USB drive.  The IT staff inserted the drive into a computer with up-to-date antivirus software.  The software identified three malware hits.  Two were for common malware and one for a sophisticated malware.  The sophisticated malware was discovered on two engineering workstations. This incident was reported in a newsletter from the U.S. industrial Control Systems Cyber Emergency Response Team. (ICS-CERT). | Malware was discovered at a U. S. power plant when an employee was experiencing issues with his USB drive.  Upon examination of the drive,  three hits for malware were detected.  Two for a common malware and one for a sophisticated malware.   Two  engineering workstations were infected with sophisticated malware.
 | https://www.risidata.com/Database/Detail/industrial_control_system_hacked_using_backdoor_posted_online | Industrial Control System Hacked Using Backdoor Posted Online | 2012 | Confirmed | United States | Other | Hackers gained unauthorized access to the industrial control system of a New Jersey air conditioning company using a backdoor vulnerability that was posted online.  The hackers breached the ICS network through a backdoor in its Niagara AX ICS system made by Tridium (a Honeywell Company) giving them access to the controls for the company’s heating and air conditioning. The system was password protected, however, the backdoor through the IP address required no password allowing direct access to the control system. Forensic logs revealed that the system was accessed from multiple IP addresses in and outside the United States. A memo was published from the FBI’s office regarding this incident.   According to the Tridium website, more than 300,000 Tridium Niagara AX Framework systems are installed worldwide.  According to Ars Technica, a search of Shodan in early 2012 security researcher Billy Rios uncovered more than 20,000 of the Niagara systems connected to the internet. | Hackers gained unauthorized access to the industrial control system of a New Jersey air conditioning company using a backdoor vulnerability.
 | https://www.risidata.com/Database/Detail/computer_virus_targets_saudi_arabian_oil_company | Computer Virus Targets Saudi Arabian Oil Company | 2012 | Confirmed | Saudi Arabia | Petroleum | Saudi Arabia’s national oil company, Aramco, said that a cyber attack damaged approximately, 30,000 computers.  The attack was aimed at stopping oil and gas production in Saudi Arabia.  The company shut down its main internal network for more than a week.  The computer virus, Shamoon, spread through Amarco's network and wiped computers' hard drives clean.  Fortunately, damage was limited to office computers and did not affect systems software that would impact technical operations.  Hackers from a group called “Cutting Sword of Justice” claimed responsibility for the attack.  Their motives were political.  The virus gave them access to documents on Aramco’s computers.  They threatened to release documents, but did not. | Approximately, 30,000 computers were affected by the Shamoon virus attack.  The virus spread through Aramco's  network and wiped computers' hard drives clean.  Fortunately, damage was limited to office computers and no technical operations were impacted.
 | https://www.risidata.com/Database/Detail/computer_malfunction_causes_train_delays | Computer Malfunction Causes Train Delays | 2012 | Confirmed | United States | Transportation | A system-wide computer glitch caused train holds and delays.  The glitch affected a computer system that allows Metro's Rail Operations Control Center to determine train locations on a dynamic map and remotely control switches.  The malfunctioning computer module was replaced. | Forty-four trains were stopped at the station for about 40 minutes on July 14, 2012.  The computers also went down for about an hour  on July 15, 2012.   With fewer trains moving at the time,  controllers radioed individual trains allowing them to move while the system was still down.
 | https://www.risidata.com/Database/Detail/computer_glitch_leads_to_shutdown_of_nuclear_reactor | Computer Glitch Leads to Shutdown of Nuclear Reactor | 2012 | Confirmed | United States | Power and Utilities | One of the two nuclear reactors at the Susquehanna Nuclear Powerplant was shut down because a computer system that controls the reactor’s water level was not functioning properly.  The reactor was shut down manually by operators when they identified the malfunction.  The cause of the control system malfunction is being investigated. | The Unit 2 reactor at the Susquehanna Nuclear Power Plant was safely shut down after operators discovered that  a computer system that controls the reactor’s water level was not functioning properly.
 | https://www.risidata.com/Database/Detail/trains_shut_down_due_to_computer_malfunction | Trains Shut Down Due to Computer Malfunction | 2012 | Confirmed | United States | Transportation | There was a  failure of a device in the information management system network.  As a result, controllers in the system’s Rail Operations Control Center could not detect the location of trains in the network or remotely control switches.   Trains came to a complete stop stranding commuters in trains and the stations throughout the District of Columbia, Maryland and Virginia. Forty four trains were halted for about 40 minutes.   Trains were also briefly halted about 10 hours later. | Forty four trains were stopped for about 40 minutes.  Passengers were stranded in trains and stations throughout the District of Columbia, Maryland and Virginia.
 | https://www.risidata.com/Database/Detail/computer_virus_infects_three_london_hospitals | Computer Virus Infects Three London Hospitals | 2008 | Confirmed | United Kingdom | Other | A computer virus infected three hospitals.  The computer systems were shut down for 24 hours.  Manual back up systems were used while the computer systems were restored.   According to Sophos expert, Graham Cluely,  a variant of the Mytob worm infected the hospitals. | Three hospitals were infected with the Mytob worm.  Computer systems were shut down for 24 hours.
 | https://www.risidata.com/Database/Detail/malware_shuts_down_hospital | Malware Shuts Down Hospital | 2011 | Confirmed | United States | Other | Malware was found in a Georgia hospital.  Computer interconnectivity was affected.  The hospital had to declare “total diversion” status for two of its campuses.  A “total diversion” status means that ambulances had to divert patients to other hospitals, except for extreme emergencies.  The hospital was forced to use a manual “runner” system which requires a courier to deliver important records.  None of the databases were affected.  Medical records were not compromised. | The hospital had to divert patients to other hospitals nearby.  The loss of computer communications within the hospital forced the use of couriers to deliver important documents. Databases and patient records were not compromised.
 | https://www.risidata.com/Database/Detail/computer_glitch_stops_trains | Computer Glitch Stops Trains | 2012 | Confirmed | The Philippines | Transportation | A computer glitch cause the Light Rail Transit Line 2 to stop.  The line was down for 30 minutes. | The glitch stopped rail line operations for 30 minutes.
 | https://www.risidata.com/Database/Detail/computer_glitch_causes_shutdown_of_2_amusement_park_rides | Computer Glitch Causes shutdown of 2 Amusement Park Rides | 2012 | Confirmed | United States | Other | A glitch in the computer system caused the Mummy ride to stop.  Riders were evacuated safely and there were no injuries.  The E.T. ride also had technical problems.  Riders were evacuated safely and there were no injuries. | Two rides were shut down.
 | https://www.risidata.com/Database/Detail/wastewater_treatment_district_hacked | Wastewater Treatment District Hacked | 2012 | Confirmed | United States | Water/Waste Water | The former chief financial officer of the Key Largo Wastewater Treatment District, Salvatore Zappulla,  has been arrested  and charged with hacking the district’s computer system.  Zappulla faces 21 felony counts including 13 counts of computer crime with intent to defraud, 7 counts of modifying information without authority and 1 count of deleting information from the  district’s computers.  Zappulla's contract was not renewed in December 2011.  The facility’s IT manager, Paul Christian, was suspicious when he discovered emails addressed to Zuppulla's personal email account during a routine check of the email system in February.    Zappulla allegedly used the login and password information of current district employees to access the district’s computer system.   When questioned by police, Zappulla bragged that he was able to prove that the district’s computers were not secure.  Salvatore Zappulla pleaded not guilty and is scheduled to go to trial the week of July 16, 2012. | The Key Largo Wastewater Treatment District was hacked by a former employee resulting in modification and deletion of files.  The former chief financial officer  allegedly used the login and password information of current district employees to access the districts computer system  remotely.
 | https://www.risidata.com/Database/Detail/computer_glitch_causes_shutdown_of_airport_people_mover | Computer Glitch Causes Shutdown of Airport People Mover | 2011 | Confirmed | United States | Transportation | A computer malfunction caused the shutdown of the Sacramento International Airport automated people mover.  The people mover that connects the airport’s Terminal B with the jet concourse building was closed for 2 hours.  Technicians shut down the people mover when system computers crashed.  Representatives from Bombadier Transportation, the manufacturer of the people mover will be investigating the cause of the failure.  Fliers were required to walk 6 minutes on an elevated outdoor ramp to and from their planes.
 | https://www.risidata.com/Database/Detail/computer_glitch_causes_airplane_plunge | Computer Glitch Causes Air plane  Plunge | 2008 | Confirmed | Australia | Transportation | A Qantas Airbus A330 flight plunged suddenly off the west coast of Australia in October 2008.  The incident left 110 people injured.  The plunge was blamed on  on the failure of the flight control computers and one of the A330s air data internal reference units (ADIRU).  The flight control commuters reported erroneous data which resulted in command for the aircraft to pitch downward. | Two systems of the computerized flight control of the Airbus A330 were blamed for the sudden plunge of a Qantas flight. The computers misinterpreted data causing the plane to pitch suddenly.  There were 110 people injured as a result.
 | https://www.risidata.com/Database/Detail/car_manufacturer_infected_with_computer_virus | Car Manufacturer Infected with Computer Virus | 2008 | Confirmed | Japan | Automotive | The Ministry of Economy, Trade and Industry (METI) reported that a major car manufacturer in western Japan was infected with a computer virus in 2008.  A system controlling the production line operations was infected when additional computers were connected to a control-system network.  Approximately 50 computers were infected.  Handling capacity was reduced, but there was no production shut-down. | A major car manufacturer control system was infected with a virus resulting in a reduction of handling capacity.  Approximately 50 computers were infected when additional computers were connected to a control system network.
 | https://www.risidata.com/Database/Detail/south_houston_water_treatment_plant_hack | South Houston Water Treatment Plant Hack | 2011 | Likely But Unconfirmed | United States | Water/Waste Water | A hacker claims to have penetrated the network of a South Houston water treatment plant.  His intent was to expose vulnerabilities in critical industrial controls and show how easily they can be compromised.  Using the name “prof”, the hacker posted on Pastebin November 18, 2011 that he gained access the software used to manage several of South Houston’s water plants.   Included in his post were links to pictures showing the privileged access he gained to the SCADA software. He said that he did not tamper with the software or any of the machines controlled by it. The hacker described using an easy-to-crack three character password that provided access to Siemens Simantic HMI software.  The City of South Houston did not issue a response.  Siemens is working with the U. S. Department of Homeland Security to investigate the incident.  “A Siemens spokesman could not confirm that the hack in South Houston, Texas, took advantage of  a default password used by the application, or one configured by officials in South Houston.  However, he acknowledged that older versions of the WinCC application do use three character default passwords.” | No actions were taken after gaining access.  However, the hacker believes his level of access would have allowed him to “play with a few settings; turn off components, and lock people out of the remote access service for a time”.
 | https://www.risidata.com/Database/Detail/steel_plant_infected_with_conifcker | Steel plant infected with Conficker | 2011 | Confirmed | XXX | Metals | On February 6, 2011, the ALSPA system stopped.  An investigation revealed that there was a Conficker virus infection in all machines of the ALSPA system.   The worm spread throughout the power plant automation network (and probably in other automation networks, but the investigation was limited to the power plant due to budget constraints).  The virus flooded the network with unwanted packets and caused an instability in the communications between PLC’s and supervisory stations and freezing most of the supervisory systems.   The automation team cleaned the infected machines, but the virus returned.  The Alston team installed the Windows Service Pack II on all machines in the ALSPA system.  After cleaning, the system returned to work well disconnected from PI.  The worm infected the PI machine and the “SGE” network, but was removed without problems.  All systems returned to work well while the external networks are disconnected.  When these networks are reconnected, the malware returns.  Due to this, the automation team decided to keep these external networks disconnected.  Since the infection began, the company is paying monthly fines to government agencies because critical reports (such as environmental control, for example) were not being sent. | The Conficker virus infected all machines of the ALSPA system.   The worm spread throughout the power plant automation network.  The virus flooded the network with unwanted packets and caused an instability in the communications between PLC’s and supervisory stations and freezing most of the supervisory systems.
 | https://www.risidata.com/Database/Detail/steel_plant_infection_with_ahack_worm | Steel Plant infection with A hack Worm | 2008 | Confirmed | Brazil | Metals | A former contractor used a G3 modem connection to have internet access inside the power plant. This infected the Power and Blast Furnace Plants with the Ahack worm.  The worm spread throughout the power plant automation network.  The flood of unwanted packets made the communication between the PLCs and the supervision stations unstable resulting in a loss of view.  The worm paralyzed the Windows Operating Systems on some machines.  The loss of view caused occasional stops and restarts of the  SCADA systems.  The worm infection caused a loss of production/operation and ultimately, financial loss. | The Ahack worm spread throughout the power plant automation network. The resulting loss of view caused occasional stops and restarts of the  SCADA systems.  The worm infection caused a loss of production/operation and ultimately, financial loss.
 | https://www.risidata.com/Database/Detail/hacker_froze_operations_at_pharmaceutical_company | Hacker froze operations at pharmaceutical company | 2011 | Confirmed | United States | Pharmaceutical | Jason Cornish, a former IT employee of Shionogi, Inc. , gained unauthorized access to the computer network.  Cornish used a Shiongi user account to access a company server.  Once the server was accessed, Cornish took control of a piece of software that he had secretly installed on a server weeks before.  He used this installed program to delete the contents of each of 15 “virtual hosts” on Shionogi's computer network.  The virtual hosts housed the equivalent of 88 different computer servers.  The deleted servers housed most of Shionogi's American computer infrastructure.  The attack effectively froze operations for a number of days.  The company suffered at least $800,000 in losses.   In July, 2011, Jason Cornish was arrested by FBI agents.  He was charged with “knowingly transmitting computer code with the intent to damage computers in interstate commerce”. He faces a maximum potential penalty of 10  years in prison and a $250,000 fine. | The deleted servers housed most of Shionogi's American computer infrastructure.  The attack effectively froze operations for a number of days.  The company suffered at least $300,000 in losses.
 | https://www.risidata.com/Database/Detail/control_system_failure_caused_phosphine_leak | Control system failure caused phosphine leak | 2011 | Confirmed | India | Chemical | The chemical reaction control system failed.  As a result, there was a leak of phosphine gas.  Three people died and over two dozen people were sickened by the gas. | The phosphine gas leak caused the death of 3 people and sickened over two dozen others.  The incident is being investigated.
 | https://www.risidata.com/Database/Detail/control_system_failure_causes_bridge_delays | Control system failure causes bridge delays | 2011 | Confirmed | Guyana | Transportation | Problems with the bridge control system while opening and closing the bridge resulted in delays for motorists.  The bridge typically closes for 90 minutes each day, but the control system problem caused a two-hour delay. Hundreds of people and their vehicles were stranded for more than 4 hours on both sides of the Berbice River. | Hundreds of people and their vehicles were stranded for more than 4 hours on both sides of the Berbice River.
 | https://www.risidata.com/Database/Detail/computer_glitch_causes_bart_train_service_shutdown | Computer glitch causes BART train service shutdown | 2011 | Confirmed | United States | Transportation | BART train service was shutdown due to a computer glitch.  An initial assessment of the problem was that a network router issue was incorrectly displaying data at the operations control center.  Because the control center could not monitor the trains, the control center managers decided to off-load passengers.   The train control computers did not fail , therefore, the computer glitch was not considered a safety issue. | The Bay Area Rapid Transit (BART) system was shut down for about 4 hours due to a computer glitch.
 | https://www.risidata.com/Database/Detail/software_failure_contributes_to_rail_car_fire | Software failure contributes to rail car fire | 2007 | Confirmed | United States | Transportation | A rail car fire started when a system failure caused an electrical surge.  The software used to monitor performance did not detect the overheating.   Steven A Feil, the chief operating officer of the Metro told the Metro board of directors that the train’s  brake resistor grid, which checks various subsystems and voltage, overheated and caught on fire after the monitoring software malfunctioned.   No one was hurt in the fire. | A rail car fire broke out when a system failure caused an electrical surge and the monitoring software did not detect the overheating.   No one was hurt in the fire.
 | https://www.risidata.com/Database/Detail/computer_glitch_causes_ride_shutdown | Computer Glitch Causes Ride Shutdown | 2011 | Confirmed | United States | Transportation | The computer systems on the Skywheel ride were not communicating with each other resulting in an error message that stopped the wheel from spinning.  The ride was shut down for 17 hours. Passengers were unloaded safely. | A computer glitch cause the Skywheel to stop turning resulting in a 17 hour shutdown.
 | https://www.risidata.com/Database/Detail/computer_glitch_causes_delayed_and_canceled_flights | Computer glitch causes delayed and cancelled flights | 2011 | Confirmed | United States | Transportation | A United Airlines computer outage caused the cancellation of 36 flights and delays of about 100 flights worldwide and in the United States.  The outage was caused by “a connectivity issue and United’s back-up system didn’t implement properly”  according to United spokeswoman Mary Clark. | The computer glitch resulted in 36 cancelled flights and about 100 delayed flights.
 | https://www.risidata.com/Database/Detail/control_system_failure_causes_shutdown_of_liquified_natural_gas_termin | Control system failure causes shutdown of liquified natural gas terminal | 2010 | Confirmed | United States | Power and Utilities | A “sensor system malfunction” caused a shutdown of the LNG terminal for about 30 minutes. The National Response Center was notified. | The LNG terminal was shutdown for about 30 minutes.
 | https://www.risidata.com/Database/Detail/circuit_card_shuts_down_nuclear_plant | Circuit card shuts down nuclear plant | 2011 | Confirmed | United States | Power and Utilities | The Watts Bar Nuclear Plant’s Unit 1 reactor shut down. A malfunction in a circuit card on a newly installed computer system that assists the operation of the plant’s turbine.  The malfunction caused the turbine to trip causing the reactor to shut down.  The cause of the trip is under investigation. | The unplanned shutdown lasted for 3 days.
 | https://www.risidata.com/Database/Detail/computer_glitch_blamed_for_train_signalling_failure | Computer glitch blamed for train signalling failure | 2011 | Confirmed | Australia | Transportation | A computer glitch was blamed for a failure at the Sydenham signalling complex.  The computer failure prevented a switch to a backup signalling system.  As a result, all trains were given a red signal and stopped.  Approximately 100,000 commuters experienced delays throughout the day.    The trains couldn't | Rail service on six lines stopped when signalling equipment failed.  Commuters experienced delays for more than nine  hours. | Rail service restored after system was rebooted.
 | https://www.risidata.com/Database/Detail/wrong_signal_shuts_down_cooling_systems | Wrong Signal Shuts Down Cooling Systems | 2009 | Confirmed | United States | Power and Utilities | Duke Energy offers an energy saving program to its customers called the Power Management program.  Participants have a box installed on their air conditioning unit that cycles the cooling system off for a few  minutes during peak usage times.  Approximately 18,000 customers participating in the program experienced an outage.  An incorrect signal was sent to the boxes according to a company spokesperson.  As a result, the boxes shut down instead of cycling.  The shutdown lasted for 3 hours. | An incorrect signal was sent to the energy saving air conditioning control boxes.  Instead of cycling, the units were shut down for 3 hours.
 | https://www.risidata.com/Database/Detail/emergency_flight_landing_caused_by_computer_glitch | Emergency flight landing caused by computer glitch | 2011 | Confirmed | United States | Transportation | The crew of United Airlines Flight 497 declared an emergency landing   while the plane was at 4,000 feet 6 minutes after taking off due to the automated warnings of smoke in the equipment bay.  The electronic centralized aircraft monitoring (ECAM) system indicated an autothrottle-related message, then an avionics smoke warning message and instructions to land.  The plane returned to Louis Armstrong New Orleans International Airport.  The preliminary report by the NTSB released on April 7, 2011 indicated that there was no actual smoke or fire onboard the plane. | A computer glitch in the electronic centralized aircraft monitoring system resulted in a smoke warning.  The crew  proceeded with an emergency landing.  The NTSB preliminary report indicated that there was no smoke or fire onboard.
 | https://www.risidata.com/Database/Detail/faulty_signaling_software_causes_train_delays | Faulty Signaling Software Causes Train Delays | 2011 | Confirmed | United Kingdom | Transportation | A signalling fault caused all signals to turn to red at about 1:10pm.  Five passenger services and five freight trains were shutdown between York and Northallerton.   There were delays across the country.  A temporary fix restored service to 10 stationary trains, but other trains were cancelled or disrupted.  Many passengers were stuck on the trains for hours. | Both passenger and freight trains were shutdown resulting in Delays and cancellations of train schedules.
 | https://www.risidata.com/Database/Detail/water_plant_shut_down_by_computer_glitch | Water plant shut down by computer glitch | 2010 | Confirmed | United States | Water/Waste Water | A computer shut down in the plant caused water processing to stop.  The water continued to be pumped into the building from the reservoir causing  the water to rise up from a manhole across the street from the plant. | Water processing stopped as a result of a computer glitch.  The continual pumping of water from the reservoir into the plant caused water to flow out of a manhole onto the street.
 | https://www.risidata.com/Database/Detail/gas_leak_caused_by_computer_malfunction | Gas Leak Caused by Computer Malfunction | 2010 | Confirmed | United States | Power and Utilities | A computer malfunction caused gas pipelines to be flooded.  The levels of gas was higher than expected causing concern with the area residents.  The elevated gas levels activated safety systems to relieve the pressure.   However,  12 relief valves were blown. | A computer malfunction led to  gas lines becoming  flooded. Though safety systems were activated, 12 relieve valves were blown. | Kansas Gas Service replaced the blown relief valves.
 | https://www.risidata.com/Database/Detail/computer_glitch_prevents_return_of_gas_service | Computer Glitch Prevents Return of Gas Service | 2011 | Confirmed | Israel | Petroleum | Noble energy had permission to stop the supply of gas to increase the pressure to enable the production of more natural gas.  The 12 hour operation was successful.  However, a computer glitch temporarily prevented the return of gas service. | A computer glitch delayed the restart of gas service after a planned shutdown.
 | https://www.risidata.com/Database/Detail/san_bruno_pipeline_explosion | San Bruno Pipeline Explosion | 2010 | Confirmed | United States | Power and Utilities | On September 9, 2010, a natural gas pipeline ruptured in a residential area.  Approximately 47.6 million standard cubic feet of natural gas was released.  The rupture created a crater approximately 72 feet long by 26 feet wide.  The gas ignited and the fire destroyed 37 homes and damaged 18.  Eight people were killed and many were injured and many evacuated the area.   Just before the accident, PG&E was working on their uninterruptible power supply (UPS).  The power supply from the UPS to the SCADA system malfunctioned.  The electrical signal to the regulating valve was lost resulting in the valve moving from partially open to full open position.  The pressure increased and ultimately the pipeline ruptured. | On September 9, 2010, a natural gas pipeline ruptured in a residential area.  Approximately 47.6 million standard cubic feet of natural gas was released.  The rupture created a crater approximately 72 feet long by 26 feet wide.  The gas ignited and the fire destroyed 37 homes and damaged 18.  Eight people were killed and many were injured and many evacuated the area.  Clean up could take 3-4 weeks.
 | https://www.risidata.com/Database/Detail/power_failure_leads_to_sewage_spill_in_washington | Power Failure Leads to Sewage Spill in Washington | 2010 | Confirmed | United States | Water/Waste Water | A power failure shut down water-treating equipment at the Mabton water treatment plant.   A computer designed to detect loss of power failed to operate.  As a result, it failed to automatically switch the plant to a back up generator and alert the plant operator. The problem went undetected for two days.  By that time, 370,000 gallons of raw sewage had been dumped into the Yakima River.  Joye Redfield-Wilder, a spokeswoman for the state Department of Ecology said that it’s probably the largest municipal waste water accident in Eastern Washington in the past 20 years. | A computer system failed to detect the loss of power at the Mabton water treatment plant.  The back up generator was not automatically switched on as expected. 370,000 gallons of raw sewage spilled into the Yakima River.  The spill did not affect drinking water or hurt the wildlife.   The river quickly diluted the sewage.  However, the spill could have exposed people  to E.coli, parasites, bacteria and viruses that cause nausea and diarrhoea and other stomach problems, a few of them fatal.
 | https://www.risidata.com/Database/Detail/faulty_train_signal_sends_train_on_collision_course | Faulty Train Signal Sends Train on Collision Course | 2010 | Confirmed | United Kingdom | Transportation | A tube train was sent into the path of another train during rush hour.  Transport for London said a “signal irregularity” caused a westbound train to end up on the eastbound track heading directly towards another train in Plaistow station.  A senior Tube source said: ‘A signal fault like this has all the potential for a major disaster’.  Shortly after leaving the station, the driver realized that that she was going the wrong way and slammed on the breaks.   Services were suspended for about six hours while an investigation took place.  A fault was found with the signalling equipment at Plaistow station. | A tube train was sent into the path of another train during rush hour.  Services were suspended for about six hours while an investigation took place.  A fault was found with the signalling equipment at Plaistow station.
 | https://www.risidata.com/Database/Detail/computer_software_faults_may_have_caused_chinook_helicopter_crash | Computer Software Faults May Have Caused Chinook Helicopter Crash | 1994 | Confirmed | United Kingdom | Transportation | The Chinook Mark 2 helicopter crashed on June 2, 1994 killing all 29 people on board.  Initially, the pilots were blamed for the incident.  Later, it was concluded that the exact cause of the accident was impossible to establish.  The BBC received internal Ministry of Defense (MoD) documents written nine months before the crash that describe the  Chinook engine control computer software as “positively dangerous”.  The report was written by a senior engineering officer at the MoD Aircraft Testing Center at Boscombe Down.  The pre-crash report stated that a hazard analysis identified the engine software as “safety critical”  and stated that “any malfunctions or deisgn errors could have catastrophic effects”.  In addition, “21 category one and 153 category two anomalies have been revealed.  One of these | The Chinook Helicopter crashed killing all 29 people on board.  New evidence casts doubt on the airworthiness of the helicopter.   A report prepared nine months before the crash indicated there may be problems with the engine control computer software.
 | https://www.risidata.com/Database/Detail/computer_glitch_causes_water_outage | Computer Glitch Causes Water Outage | 2010 | Confirmed | United States | Water/Waste Water | Two water tanks emptied leaving residents without water for about an hour and 15 minutes.  A  computer indicated that the two tanks were full.  The tanks were actually dropping and not refilling.  There is an alert system to detect low water levels, but there was no warning. | Water level monitors did not detect the dropping water levels.  Two water tanks went dry and residents were without water for an hour and 15 minutes. | A redundancy system will be added to provide a secondary monitor in the event that the main one fails.
 | https://www.risidata.com/Database/Detail/scada_system_collapse_leads_to_tunnel_closure | SCADA System Collapse Leads to Tunnel Closure | 2008 | Confirmed | Ireland | Transportation | The SCADA safety system responsible for height detection of vehicles, fire suppression mechanisms, closed circuit television and emergency lighting collapsed.   The tunnel was closed and caused traffic chaos.   Due to repeated SCADA failures, the system will be replaced at a cost of several million euro. | The SCADA failure led to the closure of the Dublin Port Tunnel.  The result was traffic chaos.  The system will be replaced due to several SCADA system failures. | The SCADA system will be replaced at a cost of several million euro.
 | https://www.risidata.com/Database/Detail/malware_a_factor_in_spanair_plane_crash | Malware a Factor in Spanair Plane Crash | 2011 | Confirmed | Spain | Transportation | On August 20, 2008  Spanair flight 5022 crashed just after take-off from Madrid-Barajas International Airport killing 154.  There were 18 survivors.  A preliminary investigation by the US National Transportation Safety Board indicated that the plane had taken off with its flaps and slats retracted and no alarm had been heard to warn of this.  The systems delivering power to the take-off warning system had failed.  Two earlier events were not reported by the automated system.  The malware detected on the Spainair central computer system was a Trojan.  It may have played a role in the crash by causing the computer to fail to detect three technical problems with the aircraft.  If the problems were detected, take-off may have been prevented. | A Trojan infected computer may not have detected three technical issues that caused the plane to crash shortly after take-off.  The crash resulted in the death of 154 passengers.  Eighteen passengers survived.  The final accident report is expected in December 2010.
 | https://www.risidata.com/Database/Detail/scada_water_system_fails | SCADA Water System Fails | 2010 | Confirmed | United States | Water/Waste Water | The overall system is linked to a SCADA which notifies on-call system operators when the pumps have failed or if water is too low in the storage tanks.  For some unknown reason, SCADA did not alert personnel to the low water levels or pump failure alerts.  The holding tank can fluctuate between 17 and 21 feet of water.   The water level was showing at 6 inches of water in the bottom of some of the storage tanks.  Nine of the 15 tanks were critical.  The failure was between the water treatment plant and the water collection well.  Residents woke up to dry taps. | Lake Havasu City residents had little or no water flow.
 | https://www.risidata.com/Database/Detail/computer_glitch_causes_monongahela_incline_shut_down | Computer Glitch Causes Monongahela Incline Shut Down | 2010 | Confirmed | United States | Transportation | The Monongahela Incline was shut down due to a computer issue.  The computers are old.  The Port Authority of Allegheny County officials said that the agency does not have the money to permanently repair or replace it. | The Monongahela Incline was shut down.  Shuttle bus service was provided between the lower and upper incline stations.
 | https://www.risidata.com/Database/Detail/failed_sensor_on_wind_turbine_caused_shower_of_ice_shards | Failed Sensor on Wind Turbine Caused Shower of Ice Shards | 2008 | Confirmed | United Kingdom | Power and Utilities | A sensor that was supposed to turn off the wind turbine under icy conditions did not operate as expected.  Ice formed on the blades and  showered nearby homes with large chunks of ice. | Ice  formed on the turbine blades.  The sensor failed to stop the turbine and shards of ice (some 2 feet long) showered nearby homes.  Operators turned off the turbine. | New equipment will be installed.  It will detect ice formation on the blades and stopped the turbine.
 | https://www.risidata.com/Database/Detail/malware_targets_uranium_enrichment_facility | Malware Targets Uranium Enrichment Facility | 2010 | Confirmed | Iran | Power and Utilities | Anti-virus specialists in Belarus discovered a worm, known as Stuxnet, that propagates by exploiting a previously unknown Windows vulnerability.  Once the machine is infected, a Trojan looks to see if the computer is running Siemens’ Simatic WinCC or PCS 7 software. The malware then automatically uses a default password that is hard-coded into the software to access the control system’s Microsoft SQL database. The password has been available on the Internet for several years.  An estimated 10,000 machines, mostly in US, Iran, Iraq and Indonesia, reported infections within the first week.    Iranian sources confirmed that the Stuxnet malworm shut down uranium enrichment at Natanz for a week from Nov. 16 to 22, 2010.  The centrifuge spinning speed was fluctuating  without the monitors detecting any malfunction.  The International Atomic Energy Agency (IAEA) director, Yukiya Amano, reported the shutdown to the IAEA board  in Vienna on Tuesday, Nov. 23, 2010. The Director of Iran’s Nuclear Energy Commission, Ali Akbar Alehi, said “Fortunately, the nuclear Stuxnet virus has faced a dead end”. | Uranium enrichment was shut down for at least one week in November 2010. | This incident largely remains under investigation at this time
 | https://www.risidata.com/Database/Detail/Switch_Malfunction_Causes_Sewage_Spill | Switch Malfunction Causes Sewage Spill | 2007 | Confirmed | United States | Water/Waste Water | A malfunctioning air compressor power switch is believed to be the cause of a sewage spill into Lookingglass Creek.  The switch operates a sensor that tracks sewage levels in the pump station’s well.  Because of early morning failure, the sewage was not pumped through a pressure line and was allowed to  spill into the creek.  The amount of material spilled into the creek is unknown. | As a result of a switch malfunction, sewage was not pumped into a pressure line and was allowed to spill into Lookingglass Creek. The switch was relatively new and was installed less than 45 days prior to the incident.
 | https://www.risidata.com/Database/Detail/scada_communictions_problems_cause_breakdown_in_water_supply | SCADA Communications Problems Cause Breakdown in Water Supply | 2007 | Confirmed | United States | Water/Waste Water | Communication problems with the decentralized SCADA systems caused an 8 hour breakdown in part of the drinking water supply in Fort Worth.  All control of the pumps and stock tanks was lost. | There was an 8 hour breakdown in part of the drinking water supply in Fort Worth
 | https://www.risidata.com/Database/Detail/trans-alaska_pipeline_spill | Trans-Alaska pipeline spill | 2010 | Confirmed | United States | Petroleum | A spill from the trans-Alaska pipleline totaled about 5,000 barrels, making it the third-largest spill from the 800-mile pipeline.  Alyeska Pipeline Services Co. kept the pipeline shutdown for 3 days after discovering  the spill at Pump Station 9 near Delta Junction.  Alyeska was testing it fire command system when power at the pump station failed.  Power was switched from the electrical grid to a battery system.  The pipeline has relief valves that open to prevent pressure from increasing inside.  They opened and oil flowed into a partially filled tank.  A control circuit in the battery system failed to close the relief valve and oil filled the tank and overflowed into the secondary containment area.  The containment area is lined with an impermeable liner.  No oil escaped from the area. The pipeline reopened on May 28, 2010.  The pipeline was shut down for 79 hours. | About 5,000 barrels of oil spilled from the trans-Alaska pipeline.  The pipeline was shut down for 79 hours. The disruption resulted in $45 million/day in North Slope production and about $13 million in state revenue. There is a crew of about 125 people on site to manage the clean up and restart. | Clean up of oil spill.
 | https://www.risidata.com/Database/Detail/japanese_nuclear_company_virus_attack | Japanese Nuclear Company Virus Attack | 2005 | Likely But Unconfirmed | Japan | Power and Utilities | Confidential data for several Japanese nuclear plants was leaked on to the internet when a worker’s computer loaded with  file-swapping software was attacked by a virus.  The 44 megabytes of files containing inspection forms, reports and manuals used from 2003 to 2005 probably appeared on the internet in March.  Company officials were not aware of the leak  until June .   The file-sharing software, whinny was loaded on a worker’s personal computer, which contained files from Tokyo-based affiliate Mitsubishi Plant Engineering Corp.  The virus that infected whinny, sent those files to the Net.   The information was from seven Japanese electric power companies and four other utility firms.  Reports said the leak was mainly reports from inspections at two nuclear reactors run by Kansai Electric Power Company.  The information was confidential, but did not include anything about nuclear materials. | Confidential inspection reports from two Japanese nuclear plants were posted on the internet. | Mitsubishi Electric has submitted a formal report to the Economy, Trade and Industry Ministry and has taken steps to prevent reoccurrence, the spokesman said.
 | https://www.risidata.com/Database/Detail/cyber_attack_on_texas_electricity_provider | Cyber Attack on Texas Electricity Provider | 2010 | Likely But Unconfirmed | United States | Power and Utilities | An investigation by Local 2 uncovered details about a so-called “cyber attack” on the Lower Colorado River Authority’s (LCRA)computer system.  A confidential e-mail obtained by local 2 describes the details of how a “single IP address in China” tried 4,888 times to log in to the Lower Colorado River Authority’s computer system.  The Electricity Reliability Council of Texas reports all of the login attempts failed according to the email.  In the email the incident was described as a “suspected sabotage event” and also stated that the FBI was notified.  Officials with the LCRA would “neither confirm, nor deny” the incident or the content of the email. | A single IP address in China tried to log in to the Colorado River Authority’s computer system.  All 4,800 attempts failed.
 | https://www.risidata.com/Database/Detail/computer_glitch_caused_planes_altitude_to_drop | Computer glitch caused plane’s altitude to drop | 2009 | Confirmed | Australia | Transportation | On Qantas flight 72, the pilot switched off autopilot shortly after taking off.  One of the Airbus A330-303’s air data inertial reference units (ADIRUs) “started providing erroneous data” according to a report issued by the Australian Transportation Safety Bureau.  The ADIRU captures and relays air data sensor information such as position and altitude which is then fed into the flight control computers.  Two of the three AIRDU’s functioned properly, however,the plane’s flight control computers had failed to filter data regarding the wing’s motion. “The computers subsequently commanded the pitch-down movements” the report said. As soon as the autopilot system was disconnected, failure warnings started.  Soon after, the aircraft suddenly pitched nose-down and fell around 650 feet.  Shortly after correcting the 37,000 feet fall in altitude, the aircraft pitched nose down again descending 400ft.  The sudden drops in altitude cuased 11 passengers to be seriously injured.   The ATSB identified problems with two secondary computers' ability to execute commands sent from the primary computer.  The investigation will continue. | Eleven passengers on board were seriously injured when the plane’s altitude dropped twice during the flight.
 | https://www.risidata.com/Database/Detail/pc_sality.en_virus_infects_dcs_servers_and_historians | PC_SALITY.EN Virus Infects DCS Servers and Historians | 2009 | Confirmed | South Africa | Chemical | OPC Services stopped running on two OPC servers following a hardware & application software upgrade of 4 OPC client machines that were connected to the servers.  The client upgrades were aborted and the old machines were reinstalled.  The OPC servers could not be rebooted.  Upon investigation the PE_SALITY virus was found on both of the OPC servers. | Operators ran the plant “blind” for over 8 hours while engineering rebuilt both of the OPC servers. | Made the following changes: No third party software to be installed on any server if it is not approved by the vendor. No server is allowed to be integrated in network if it is not tested for viruses/virus free No remote access allowed Only used dedicated memory sticks in servers
 | https://www.risidata.com/Database/Detail/scadalalarm_pcanywhere_compatibility | SCADAlalarm & PCAnywhere compatibility | 2010 | Confirmed | United States | Power and Utilities | A new version of PCAnywhere (12.5) was purchased.  Some strange compatibility issues with SCADAlarm were discovered. When PCAnywhere 12.5 is set up as a host on Windows 2000 machines, and accessed the host over the SCADA network, everything worked fine. However, when  access was attempted from a machine through a modem, it | The incompatibility of PCAnywhere and SCADAlarm caused SCADAlarm to become non-functional.
 | https://www.risidata.com/Database/Detail/computer_glitch_causes_sewage_spill | Computer Glitch Causes Sewage Spill | 2009 | Confirmed | United States | Water/Waste Water | About 6:30 on Christmas night, sewage was diverted to due to a blockage.  A malfunction alarm that should have alerted workers at an off-site control room, but that did not occur.  As much as 2.4 million gallons of sewage was blocked from flowing into the Eastern Municipal Water District plant on Venida Alvarado, eventually making its way into the Murrieta Creek.  Workers vacuumed up about 980,000 gallons of sewage. | As much as 2.4 million gallons of sewage spilled into Murrieta Creek.  A malfunction alarm did not alert off-site workers.  The spill was not discovered for 12 hours.  The San Diego Regional Water Quality Control Board will determine if Eastern will be fined and if so, how large the fine will be.
 | https://www.risidata.com/Database/Detail/disgruntled_employee_remotely_disables_cars_using_the_webtech_plus_system | Disgruntled Employee Remotely  Disables Cars using the Webtech Plus System | 2010 | Confirmed | United States | Automotive | A man fired from a Texas auto dealership used an internet service to remotely disable ignitions and set off car horns of more than 100 vehicles sold at his former workplace.  Omar Ramos-Lopez used a former colleagues password to access the Webtech Plus system operated by Pay Technologies.  This system lets car dealers install a box under the vehicle dashboards that responds to commands issued through a central website, and relayed over a wireless pager network.  It is an alternative to repossessing the vehicle.  The dealer can disable the car’s ignition system or trigger the horn as a reminder that a payment is due.  The Texas auto center received customer complaints.  The issues subsided when Texas Auto Center reset the Webtech Plus passwords for all of its employee accounts.  The police obtained access logs from Pay Technologies and traced the activity to the IP address of Ramos-Lopez’s internet service.  This incident is the first time an intruder abused the system according to Jim Krueger the co-owner of Pay Technologies. Ramos-Lopez was charged with felony breach of computer security. | Car ignitions were disabled and horns were activated remotely in an act of sabotage.  The vehicle owners could not operate their vehicles and many missed work and had to have their cars towed.
 | https://www.risidata.com/Database/Detail/flood_warning_system_failure_causes_flooding | Flood Warning System  Failure Causes Flooding | 2009 | Confirmed | United Kingdom | Water/Waste Water | The flood warning system at the Deephope gauging station failed.  The Tima Water, a tributary of the Ettrick Water rose above the alarm level and warnings should have gone to several groups in the Ettrick Valley in the Borders.  A malfunction occurred in the handling system that is responsible for forwarding alarms as text messages.  As a result, homes and businesses were flooded without warning. | Homes and businesses were flooded without warning.  A spokesman fro SEPA said:  “We would like to apologise to Borders residents for a failure in their flood warning system and would like to reassure them the system has now been checked and the fault corrected.”
 | https://www.risidata.com/Database/Detail/scada_system_failure_causes_shut_down_of_dublin_port_tunnel | SCADA System Failure Causes Shut down of Dublin Port Tunnel | 2008 | Confirmed | Ireland | Transportation | The SCADA system, responsible for height detection of vehicles, fire suppression mechanisms, closed circuit television and emergency lighting in the Dublin Port Tunnel failed causing traffic chaos.  The National Roads Authority (NRA) expressed “serious concerns” about both the “durability and reliability” of the SCADA safety system.  An NRA spokesman described the shutdown as “an emergency situation”.  Adding that the system would be replaced or repaired “piece by piece” to avoid future shutdowns. | The SCADA system failure was responsible for a forced shutdown of the Dublin Port Tunnel resulting in traffic chaos.
 | https://www.risidata.com/Database/Detail/slammer_worm_hits_major_us_auto_manufacturer | Slammer Worm Hits Major US Auto Manufacturer | 2003 | Confirmed | United States | Automotive | At 5pm on Saturday, June 25, 2003 a major US auto manufacturer was hit with the Slammer worm.   A SQL patch had been available for approximately 6 months but had not been applied to the manufacturers systems.   The worm spread very quickly, ultimately affecting 17 of the manufacturer’s plants.  The event lasted for approximately 8 hours before it was contained.  Limited infrastructure firewalls did nothing to prevent the spread.  Ultimately, IT was forced to detach the unprotected telecom infrastructure which appeared to be the primary mode of spread between plants. | 17 manufacturing plants affected.   More than 1,000 computers had to be “rebuilt”.  Event was estimated to cost the company $150 Million USD. | The manufacturer conducted an audit of all plants for external unwatched/unprotected data connections.  One plant alone was found to have over 400 violations.   They conducted security awareness training in all plants.
 | https://www.risidata.com/Database/Detail/puget_sound_sewage_spill | Puget Sound Sewage Spill | 2009 | Confirmed | United States | Water/Waste Water | A sensor on a valve malfunctioned and resulted in the release of 10 million gallons of untreated wastewater into Puget Sound.  In the early morning at the West Point plant in Discovery Park, a mishap with a sensor sent the roughly 5-to-1 mixture of stormwater and sewage through a pipe into the shallow waters of the Elliot Bay.  Ecologists expect short term impacts.  However, the untreated waste would be higher in nitrogen which can contribute to fish-killing low-oxygen zones like the one in Hood Canal. | A  faulty sensor is responsible for the release of 10 million gallons of untreated wastewater in Puget Sound.  This spill is being called King County’s worst spill in decades. | Treatment plant workers plan to examine their equipment and their responses to try to prevent sewage spills.
 | https://www.risidata.com/Database/Detail/faulty_water_level_alarm_cause_of_sewage_spill | Faulty Water Level Alarm Cause of Sewage Spill | 2009 | Confirmed | United States | Water/Waste Water | A sewage spill in Lebanon Missouri was blamed on a faulty alarm.  An estimated 18,000 gallons of partially treated wastewater and sludge was released from Lebanon’s sewage plant.  The high-water-level alarm malfunctioned allowing the partially treated wastewater and sludge to be released into a nearby tributary of Dry Auglaise Creek and ultimately into the creek.  The overflow began at approximately 5 a.m. and continued until shortly after 8 a.m. The incident was reported to the Department of Natural Resources (DNR).    An inspector was dispatched to the site to determine the extent of environmental damage to the creek. The the discharge of partially treated sewage from sanitary sewer collection systems is considered by the DNR to be a significant threat to public health and the environment by contaminating lakes and streams that lead to serious water quality issues. | An estimated 18,000 gallons of partially treated wastewater and sludge was released into the Dry Auglaise Creek.  The overflow continued for approximately 3 hours.   The Department of Natural Resources is investigating to determine the extent of environmental damage.
 | https://www.risidata.com/Database/Detail/computer_error_causes_flight_delays | Computer Error Causes Flight Delays | 2009 | Confirmed | United States | Transportation | A software glitch caused flight delays across the country.  The problem made it impossible for airlines to enter flight plan information into the National Airspace Data Interchange Network (NADIN).  FAA personnel entered the information manually, therefore, the additional time needed led to delays.   According to Paul Takemoto, an FAA spokesman, the NADIN system was not the ultimate system fault.  Tammy Jones, an FAA spokeswoman, said the problem is being attributed to a software configuration problem with a router at the Salt Lake City facility. | A software problem caused flight delays across the country.  This was not the first serious problem.  A similar incident occurred in 2008.  See RISI incident no. 169.  The software configuration problem being attributed with a router in Salt Lake City, had been resolved after about 4 hours.
 | https://www.risidata.com/Database/Detail/computer_problems_causes_flight_delays | Computer Problems Causes Flight Delays | 2008 | Confirmed | United States | Transportation | Computer problems at a Federal Aviation Administration center outside of Atlanta caused delays at some major airports across the country, including Boston, Atlanta and Chicago.  The problem was a failure of a processing system that handles flight plans filed by the airlines before their aircraft take off.  As a result of the failure, processing was shifted to a backup system in Salt Lake City.   For several hours holds on arriving traffic at other Logan and Hartsfield.    The holds were later lifted, However, There were significant flight delays in Boston, Atlanta and Chicago airports. | As a result of the system failure, flights were delayed significantly at some major airports around the country including Boston, Atlanta and Chicago.
 | https://www.risidata.com/Database/Detail/refinery_explosion_and_fire_caused_by_non-functioning_computerized_level_mo | Refinery Explosion and Fire Caused by Non-Functioning Computerized Level Monitoring System | 2009 | Confirmed | United States | Petroleum | A large vapour cloud ignited at the Caribbean Petroleum facility near San Juan Puerto Rico.  The explosion damaged homes and businesses over a mile from the facility.  At the time of the incident a tank was being filled with gasoline from a ship docked in San Juan harbor.  Investigators have determined that a likely scenario leading to the release was an accidental overfilling of the tank.  Gasoline spilled without being detected.  The Chemical Safety Board found that on the evening of the incident, the liquid level in the tank could not be determined because the facility’s computerized level monitoring system was not fully operational.  In order to monitor the level in the tank, operators used a mechanical gauge on the tank’s exterior wall.  Therefore, as the gasoline level in the tank rose  and eventually overflowed, employees in the facility’s control room were unaware of the emergency. | Homes and businesses were damaged as a result of the blast.  Damage extended over a mile away from the facility.
 | https://www.risidata.com/Database/Detail/engineers_hack_into_los_angeles_traffic_signal_computer | Engineers Hack into Los Angeles Traffic Signal Computer | 2006 | Confirmed | United States | Transportation | Two Los Angeles traffic engineers hacked into the city’s signal system causing slowing traffic as part of a labor dispute protest.  Gabriel Murillo and Kartik Patel were able to hack into the system despite the city’s efforts to block access during a labor action.  They strategically chose their targetsl.  They knew which signals would cause significant backups.  Murillo and Patel programmed signals so that red lights would be extremely long causing grid lock.  No accidents occurred.   Murillo and Patel pleaded guilty to hacking into the city’s signal system and were sentenced to two years' probation.  As part of their plea deal, the engineers agreed to pay $6,250 in restitution and complete 240 hour of community service. | The L.A. traffic signal system was hacked.  As a result, signals at busy intersections were programmed to remain red for an extended period causing traffic backups.  No accidents occurred.
 | https://www.risidata.com/Database/Detail/computer_sabotage_at_nuclear_power_plant | Computer Sabotage at Nuclear Power Plant | 1992 | Confirmed | Lithuania | Power and Utilities | A computer programmer at the Ignalina Power Reactor Station in Lithuania introduced a virus into one of the stations computers in an attempt to sabotage a reactor at the plant by introducing a virus into the computer system.  Oleg Savchuk was arrested on a charge of premeditated sabotage.  The station shut down the same day the incident was reported, however, a spokesman said this was coincidental and had nothing to do with the computer virus.  The cooling system in the first reactor broke down.  There is some controversy surrounding the case.   There were accusations that the station management fabricated the incident to get rid of Savchuk.  There were also suggestions by station management that Savchuck may have introduced the virus and then called it to the attention of management in order to receive a bonus for solving the problem.  The infected computer system was in control of subsidiary systems,  not the reactor according to the deputy minister of Power Engineering, Saulius Kutas in a statement to the Lithuanian news media. | Nuclear plant computer was infected with a virus.  There was a station shutdown, though it was reported to be coincidental and not caused by the virus. Oleg Savchuk was arrested for premeditated sabotage.
 | https://www.risidata.com/Database/Detail/automated_antiaircraft_cannon_malfunctions_kills_9_wounds_14 | Automated Anti-aircraft Cannon Malfunctions, Kills 9, Wounds 14 | 2007 | Confirmed | South Africa | Other | A software glitch is being blamed for an antiaircraft cannon malfunction that killed 9 soldiers and seriously injured 14 others during a shooting exercise. The cause of the malfunction is not known.  The South African National Defense Force is investigating whether a software glitch was the cause.  The anti-aircraft weapon, an Oerlikon GDF-005, which is computerized opened fire uncontrollably. | Computerized weapon fired uncontrollably during a shooting exercise killing 9 spectators and injuring 14 others.  A computer glitch is being blamed for the malfunction, though the possibility of mechanical failure has not been ruled out.
 | https://www.risidata.com/Database/Detail/wastewater_pumped_into_jones_falls | Wastewater pumped into Jones Falls | 2009 | Confirmed | United States | Water/Waste Water | An estimated 700,000 gallons of waste water overflowed from a pumping station on Sisson Street into the Jones Falls.  There was no danger to the public because a screen filtered any macropollutants out before entering Jones Falls.   The overflow was the result of a malfunctioning automated switching system that controls electricity to the station’s pumps.  City officials are not sure of the exact cause of this malfunction, but think a design flaw contributed to water backing up in the station.  Public works employees opened a valve and allowed the waste water to flow out so that the station would not be damaged.  A spokesman for the Baltimore City Department of Public Works, Kurt Kocher, said this type of overflow is uncommon. | An estimated 700,000 gallons of waste water overflowed from the pumping station into the Jones Falls.  There was no “visible” pollution of the Jones Falls because the waste water was filtered of any macropollutants.  However, public notice of the overflow was posted along the Jones Falls.  The pumping station was being refurbished.
 | https://www.risidata.com/Database/Detail/energy_company_virus_attack | Energy Company Virus Attack | 2009 | Confirmed | Australia | Power and Utilities | A virus attack on Integral Energy’s computer network forced the company to restructure all of its 1,000 desktops.  Eternal security experts were called in to rebuild all of the desktop computers to contain and remove the virus.  The malware had not affected the power grid.  Chris Gatford a security consultant from Hacklabs had conducted penetration testing on critical infrastructure said there was often “ineffective segregation” or “more typically none at all between the IT network and the network that monitors and controls the infrastructure. However a spokesperson from Integral Energy stressed that the virus attacks Microsoft products and the network doesn’t run on Microsoft and there was no way that the virus could make its way onto the grid.  The virus was the W32 Virut.CF strain which has been described as “a particularly sinister file infector” that spreads quickly and is considered difficult to remove.  Integral Energy’s computer networks were protected by a Symantec security solution, a source said.  The Symantec website states that the virus installs a back door enabling hackers to issue commands to the infected machines via an internet relay chat (IRC) channel.  According to Gatford, the antivirus software was not updated in a timely manner on some machines or the Symantec product could not detect it. Integral Energy supplies electricity to Western Sydney and Illawarra region of New South Wales distributing electricity to 2.1 million people in NSW. | Integral Energy’s computer network was infected with the W32.Virut.CF virus.  All 1,000 of its desktop computers had to be rebuilt. | Integral Energy called in a range of experts to help with the virus infection.  The company put in place recovery plans to eliminate the virus from its business systems.  An investigation is underway to determine the cause of the infection and develop a strategy to minimize risk in the future.
 | https://www.risidata.com/Database/Detail/energy_company_exposed_to_hackers_by_a_phishing_attack | Energy Company  Exposed to Hackers by a Phishing Attack | 2007 | Likely But Unconfirmed | Unknown | Power and Utilities | An energy company hired Intreidus Group to investigate an attack that exposed the company to hackers.  The investigation revealed that the company was a victim of a phishing attack that resulted in outsiders gaining control of an employees computer.   The attackers gained a level of access that would have allowed them to control, view and modify everything related to the business.   An email sent to employees at an energy company was opened by one of the employees.   The attack started with random administrative account being added in the internal network.  The primary domain controller in the system had been compromised.  The attack had originated inside the corporate network.  The machine resided on the same segment as the SCADA controllers were located.   The source of the breach was found to be a phishing attack.   The phishing email contained information claiming to be about employee benefits.  An employee opened the email attachment unaware of the malicious .chm file attachment.  The opened attachment reached out to a server the the Asia-Pacific region and release a malicious executable that gave the attackers access to the employees computer.  The attackers used a Windows DNS (Domain Name System) vulnerability as an entry point to gain control of the employees account.  Intrepidus recommended the company re-architect the outbound filtering of Internet access and pt a proxy in place for Web browsing.  In addition, segregation was advised so that no workstations sharing a critical network segment should be connected to the Internet. | The phishing attack lead to a compromise of the energy company network and access to its SCADA system.
 | https://www.risidata.com/Database/Detail/faulty_software_causes_torrens_lake_drain | Faulty Software Causes Torrens Lake Drain | 2009 | Confirmed | Australia | Water/Waste Water | Faulty software caused the gates of the Torrens Weir to open without warning.  The gates remained open for about two hours draining millions of litres of water from the lake.  An investigation revealed that alarms that would alert remote operators of a malfunction were muted.  The faulty software was purchased from Ottoway System Integration, an Adelaide-based  firm which went out of business only days after the incident.   There was no evidence of foul play. | The Torrens Lake was drained.  The water levels dropped by more than two meters.   The muddy lake bottom contained large amounts of debris.  The incident  caused a problem for businesses that relied on the lake.  Since the incident, the weir has been opened and closed manually.  One positive outcome is that the 17 tons  of debris was removed from the lake.  A report from GHD Engineering contains 11 recommendations including an overhaul of the software used to regulate the weir.  Lord Mayor Michael Harbison was adamant the system would be upgraded.
 | https://www.risidata.com/Database/Detail/computer_failure_causes_jet_crash | Computer Failure Causes Jet Crash | 2009 | Confirmed | France | Transportation | The Air France flight 447 crashed into the Atlantic Ocean.  The black boxes were not recovered, but based on physical evidence and information from automatic maintenance messages sent, it is believed that a cascade of system failures starting with the airspeed sensors  progressed to sweeping computer outages.  The Airbus A330 en route from Rio de Janeiro to Paris crashed into the Atlantic Ocean during a storm. | The Air France flight 447 crashed into the Atlantic Ocean.  All 228 passengers were killed.
 | https://www.risidata.com/Database/Detail/computer_glitch_floods_neighborhood | Computer Glitch Floods Neighborhood | 2009 | Confirmed | United States | Water/Waste Water | A computer glitch caused pumps on the city’s water tank to fail to shut down when the tank was full.  The electronic equipment used to monitor the tank gave a false reading when  the tank was full.   The pump produces 7,000 gallons per minute.  Tens of thousands of gallons of water, mud and rocks rushed through a Cedar Hills neighborhood.  The flood channel channel, built by the city, was clogged which caused water to go onto the lawn and driveway of the residential home affected.  The glitch may have been caused by a brief electrical storm the night before the flood. | Tens of thousands of gallons of water, mud and rocks streamed down a mountainside into the Cedar Hills neighborhood causing minimal damage to one home.
 | https://www.risidata.com/Database/Detail/sewage_spill_shuts_down_a_cornish_shellfishery | Sewage Spill Shuts Down a Cornish Shellfishery | 2007 | Confirmed | United Kingdom | Water/Waste Water | On June 2, 2007, a computer and penstock controlling the flow of sewage into the Truro Treatment Works failed.  This caused sewage to be directed to a storm tank that continued to fill until late on June 3, 2007 when it reached capacity and overflowed into the Truro River.   The discharge was not reported until 3 days after the spill. | Sewage spilled into the Truro River.  The shellfishery was closed for 12 days.  The local fisherman suffered financially.  South West Water was fined
 | https://www.risidata.com/Database/Detail/lightning_strikes_cause_sewage_pump_station_overflow | Lightning Strikes Cause Sewage Pump Station Overflow | 2009 | Confirmed | United States | Water/Waste Water | The sewage pump stations at Austin Run and Potomac Hills overflowed due to lightning strikes that disabled the flow transducers at both stations.  The pump transducers control the pumps that move effluent from the wells to the Aquia treatment center.  The Austin Run station overflowed approximate 2.5 million gallons into Austin Run and Aquia Creek. The Potomac Hils station overflowed approximately 55,000 gallons into Aquia Creek.  The overflow volumes were higher than normal because the telemetry system for these two stations malfunctioned and did not activate the station alarms.  The utilities plant operators were not aware of the overflow because the alarms did not activate.  The overflows were discovered when mechanics rebooted the telemetry system 2 days later. | The Austin Run station overflowed approximate 2.5 million gallons into Austin Run and Aquia Creek. The Potomac Hils station overflowed approximately 55,000 gallons into Aquia Creek. | Stations repaired, and staff is working with the telemetry contractor to determine how to prevent another malfunction. Staff spread lime to disinfect the area of the spill.
 | https://www.risidata.com/Database/Detail/paperless_chart_recorder_software_hacked | Paperless Chart Recorder Software Hacked | 2009 | Confirmed | United Kingdom | Unknown | Several installations of paperless chart recorders started to exhibit faults, like “going to sleep” or cyclic rebooting.  The recorders were exchanged with spares and returned them to the manufacturer.  There was no improvement in reliability.  It was discovered that the chart recorders contained a game based on the movie “Hunt for Red October”.  The game could not be deleted from the firmware, only locked out from the Operators.  After “lock-out”, reliability seemed to improve, but faults were never completely eliminated.  Sellarfeld decided to change the recorders for another make. | Chart recorder faults and lock-out.
 | https://www.risidata.com/Database/Detail/computer_glitch_causes_7_water_mains_to_break | Computer Glitch Causes 7 Water Mains to Break | 2009 | Confirmed | United States | Water/Waste Water | A computer glitch caused a false low pressure reading and turned on the pumps at a United Water facility.  A computer malfunction at the Troy pumping station was blamed for the 7 water main breaks in Jersey City Heights, NJ.  Two firefighters responding to an alarm at a senior center on Hague Street at noon and noticed water in the street.  They notified United Water.  There were other water main breaks at Hutton St. and Cambridge Ave, Bleecker St. and Pierce Ave., Congress St. and Shermann Ave, Thorne St. and Kennedy Blvd., Congress St. and Odgen Ave.,  and Ravine Ave.. | A boil order was issued.  Water in some places was reported to be brown. Low water pressure resulted in some residents not having water for a period of time.   Water tankers were posted at two locations in the event of a fire.
 | https://www.risidata.com/Database/Detail/tunnel_shutdown_after_fault_in_control_system | Tunnel Shutdown after Fault in Control System | 2008 | Confirmed | Australia | Transportation | At 7am, a peak travel time, the computer that controls the M5 East tunnel suddenly crashed.  The technicians in the tunnel operator’s control Arncliffe control room realised the emergency back-up server also failed.  There was no way to ensure that the rapid “deluge” system that would extinguish tunnel fires would work. The tunnel had to be shut down immediately in both directions.  The tunnel did not open for 5 hours. This incident is the 5th in a succession of computer failures that have wreaked chaos on Sydney’s road network. On February 1, 2002, just two months after it opened, a computer glitch forced the tunnel’s closure for an hour before 6am to ensure the operating systems would operate properly. Eleven months later, it closed again for 45 minutes in the afternoon peak, because its lighting systems failed. On March 9, 2004, there was a combined power failure that required the full closure of the tunnel in the morning peak. And in December that year, the closed-circuit television systems failed, shutting the tunnel again. Baulderstone Hornibrook Bilfinger Berger (BHBB), is the contractor under the M5 East Motorway Design, Construct, Maintain and Operate (DCOM) contract. | Tens of thousands of motorists were trapped in a traffic jam.
 | https://www.risidata.com/Database/Detail/hospital_hvac_hack | Hospital HVAC Hack | 2009 | Confirmed | United States | Other | A security guard for a Texas hospital was arrested for allegedly breaking into the facility’s HVAC and confidential patient information systems.  Jesse William McGraw, also known as “GhostExodus” was charged with downloading malicious code onto a computer at the Carrell Clinic to cause damage.   Wesley McGrew, an expert in control safety systems and SCADA security saw screenshots of the hospital’s HVAC system posted online by “GhostExodus”.  The screenshots showed an HMI that gave the user control over many of the hospital’s  utilities including pumps and chillers in the operating room.  According to court documents, hospital officials had experienced problems with their HVAC units and were confused as to why none of the systems alarms had gone off as programmed.  Screenshots posted by “GhostExodus” showed the HVAC window for the hospital surgery unit.  The test alarm system was switched to “inactive”. Update 2May2011:  On March 17, 2011, Jesse McGraw who admitted to hacking into the hospital’s computer systems, was sentenced to 110 months on each of two counts to be served concurrently.  In addition, McGraw was ordered to make restitution to the occupants in the building affected by his criminal conduct, specifically the W.B. Carrell Memorial Clinic, the North Central Surgery Center and the Cirrus Group. | The hospital experienced problems with the HVAC system.  The attack was a threat to public health and safety.
 | https://www.risidata.com/Database/Detail/texas_power_company_hack | Texas Power Company Hack | 2009 | Confirmed | United States | Power and Utilities | A former employee of a Texas power utility was arrested on May 28, 2009 for crippling the company’s energy forecast system.  The ex-employee, Don Chul Shin was fired from Energy Future Holdings on March 3, 2009 for performance reasons and was escorted off of the property.  However, the company failed to immediately shut off his VPN access.  Later that day, Shin’s account was used to log onto the  corporate network, e-mailing out proprietary data to a personal Yahoo account linked to Shin and modifying and deleting files.  Company logs indicated that the VPN connection originated from Shin’s IP address.  While logged onto the corporate VPN, an email was sent asking the engineering group operating the Comanche Peak nuclear reactor asking what would happen if the load were to be “increased to 99.7 percent of capacity.”  Shin was responsible for programming the models which controlled the management of EFH power generation facilities, including the Comanche Peak reactor. The company reported the sabotage on March 6, 2009. Energy Future Holdings is the corporate parent of three large Texas electric companies, including Luminent, which operates the Comanche Peak nuclear power plant. | The impact was purely financial.  The company reported that the incident cost over $26,000.
 | https://www.risidata.com/Database/Detail/environmental_southland_target_of_cyber_vandals | Environmental Southland Target of Cyber Vandals | 2009 | Confirmed | New Zealand | Other | Intermittent problems with two computers used to coordinate the telemetry and interactive voice response system (IVRS) were noticed.   Flood warning data was  collected and stored but callers had been unable to check river levels using the telephone reporting system on a few occasions because the computers stopped communicating with each other.  Environmental Southland had to reset the computers manually every time the problem occurred. An investigation revealed that there had been a series of attempts to hack into the flood warning system from email addresses with Chinese domain names.   The hackers had found a gap in the firewall, but did not have the correct passwords to cause further disruptions.  The gap in the firewall probably went unnoticed for “two or three years” according to the IT manager, Stephen Aldridge.  Environment Southland is a New Zealand government agency that protects the natural resources, including water, air and land. | The flood water levels could not be checked using the automated telephone reporting system.  Loss of staff time. | Closed gaps in firewall. New security measures were installed.
 | https://www.risidata.com/Database/Detail/limerick_nuclear_reactor_1_shut_down | Limerick Nuclear Reactor 1 Shut Down | 2008 | Confirmed | United States | Power and Utilities | The nuclear power station in Limerick experienced a shut down in the Unit 1 reactor due to a problem with the main turbine control system on the electrical side of the plant.  The Limerick nuclear power station has two 1,134 MW units numbered 1 and 2.  Unit 2 continued to operate at full power. | The unit 1 reactor shut from full power.  Unit 2 continued to operate at full power.
 | https://www.risidata.com/Database/Detail/computer_glitch_causes_overflowing_water_tower | Computer Glitch Causes Overflowing Water Tower | 2007 | Confirmed | United States | Water/Waste Water | A passer-by noticed  water pouring over the top of the water tower near Chippewa Falls Middle School in the early morning hours and called police.  Rory Olson, city water utilities manager summoned to the tower about 4:15 a.m., also found water flowing from a side vent.  City Engineer Rick Rubenzer said the problem resulted from the failure of a control on a computer system.   “Pumps filling the water tower continued to run and were not told by the computer to stop,” Rubenzer said. “The tank filled and bubbled over.” The city water tower had the exact opposite problem last Thanksgiving. In that incident, the computer system mistakenly indicated the tower was full and stopped pumping water. | The water level dropped low enough to cut service to part of the city.
 | https://www.risidata.com/Database/Detail/washington_dc_metro_accident | Washington DC Metro Accident | 2009 | Confirmed | United States | Transportation | A computerized system failed to stop an oncoming train that was operating in automatic mode.  The emergency brake was pressed and the train did not stop.  The two metro trains crashed. The train was never supposed to get closer than 1,200 feet.  Federal officials had sought to phase out the ageing fleet of trains because of safety concerns, but the transit system kept the old trains running, saying it lacked the money for new cars. The National Transportation board warned that the old fleet should be replaced or retrofitted to make it better able to survive a crash.  Neither was done. | The train wreck left 9 dead and 80 injured. | Investigation continues as to the exact cause of the control system failure.
 | https://www.risidata.com/Database/Detail/coors_brewing_shutdown | Coors Brewing  Shutdown | 2004 | Confirmed | United States | Food & Beverage | A current employee with access to the bottling control system, inadvertently changed a timer for a maintenance device on a filler.  Instead of squirting grease into the bearing every 20 minutes, it was set to once every 8 hours.  Consequently, the bearings froze.  The line that fills the bottles at 1,200 per minute stopped. | Operations were shut-down resulting in $100,000 loss. | Rockwell Software helped Coors put security into their manufacturing system that was designed primarily for efficiency and reliability.
 | https://www.risidata.com/Database/Detail/taum_sauk_water_storage_dam_failure | Taum Sauk Water Storage Dam Failure | 2005 | Confirmed | United States | Power and Utilities | On the morning of December 14, 2005, the dike at the Taum Sauk Upper Reservoir failed.  The water in the Upper Reservoir was released into the East Fork of the Black River, located upstream of the lower Taum Sauk dam.  The 1.3 billion gallons of water flowed from the Black River, through Johnson’s Shut-Ins National Park into the Lower Reservoir, and then continued to flow down the Black River to the the town of Lesterville, Missouri. The water released destroyed the home of the Johnson’s Shut-Ins State Park superintendent, flooded motorists, and significantly damaged the park, campground, and adjacent properties.  There were no fatalities, however, the Park Superintendent and his family were injured when they were swept away by the water.  According to The Federal Energy Regulatory Commission (FERC), The primary cause of the Taum Sauk breach was overtopping of the upper reservoir due to improperly maintained and installed water level monitors.  The monitors became loose and indicated the reservoir levels lower than actual levels.  It was also found that emergency back up sensors proved ineffective because they “were set at an elevation above the lowest points along the parapet wall; thus, they failed their protection role because this enabled overtopping to occur before the probes could trigger a shutdown.”  In addition, Ameren typically operated with high water levels of one foot below the top of the parapet wall, which did not take into account any possible mistakes in operation. | The damage to the Johnson’s Shut-Ins National Park was extensive.  The Taum Sauk dam failure has been called the “Worst Man-Made Disaster in the history of Missouri”. FERC fined Ameren $15 million pursuant to a settlement for the breach at Taum Sauk.  The State of Missouri sued Ameren for Actual and Punitive Damages alleging Ameren reckless in its operation.  In November 2007, the state reached a $180 million settlement with Ameren.  The settlement reached compensates the people of Reynolds County and the state of Missouri for the loss of natural resources and recreation. | The upper reservoir dam is being replaced with a roller compacted concrete dam.
 | https://www.risidata.com/Database/Detail/computer_glitch_causes_major_power_outage | Computer Glitch Causes Major Power Outage | 2007 | Confirmed | United States | Power and Utilities | A computer glitch is being blamed for a massive power outage in the Phoenix area.  Between 80,000-100,000 Salt River Project customers were affected.  The outage lasted for 20-30 minutes.  The Salt River Project has a system in place to shed load if it is unable to supply power.  That didn’t happen on the day of the outage.  There was a computer problem that thought it did and it triggered the widespread outage. | Power outage to 80,000-100,000 customers for 20-30 minutes.
 | https://www.risidata.com/Database/Detail/hacker_activates_emergency_sirens | Hacker Activates  Emergency Sirens | 2008 | Confirmed | United States | Other | A hacker with radio codes set off emergency sirens.  Police were unable to override the hackers tones and had to turn off each of six sirens by hand.  The sirens went on for 20 minutes.    It was the second incident that year.   The prior incident was on March 27, 2008.  The barrage of calls from residents knocked out the departments phone system. | The warning system was unavailable in case of a real emergency.  No arrests were made.   A reward was offered for help in an arrest.
 | https://www.risidata.com/Database/Detail/texas_road_sign_hack | Texas Road Sign Hack | 2009 | Confirmed | United States | Transportation | A hacker gained access to a digital road sign in the Austin area.  He changed the sign to read “Zombies Ahead” during the morning commute.  The sign reverted back to its original message hours later. | No damage to the sign.   Tampering with road signs is a misdemeanour in Texas, with penalties ranging from fines to jail time.
 | https://www.risidata.com/Database/Detail/power_outages_and_other_service_interruptions | Power Outages and Other Service Interruptions | 1999 | Confirmed | United States | Power and Utilities | A former computer systems administrator, Joseph D. Konopka, hacked into computers and caused power failures  in WI.  Konopka, also known as ‘Dr. Chaos',  pleaded guilty and was sentenced on 11 felony charges for crimes in WI.  He was responsible for 28 power outages and 20 other service interruptions in WI in 1999.  He was sentenced to 7 years in prison.  He was ordered to pay $436,000 in restitution and spend three years on supervised release after prison.   Konopka’s crime spree also included hiding cyanide in an underground tunnel near a subway system in Chicago.   Was also sentenced to 13 years in prison. | Twenty eight power outages and 20 other service interruptions causing about $800,000 in damage in 13 Wisconsin counties.
 | https://www.risidata.com/Database/Detail/former_system_administrator_sentenced_for_wrecking_corporate_servers | Former System Administrator Sentenced for Wrecking Corporate Servers | 2007 | Confirmed | United States | General Manufacturing | A former contract systems administrator for Pratt-Read deleted files on three of the company’s servers, knocking the servers offline critically damaging operations.  The contractor, Priyavrat Patel, accessed the system  from home during the Thanksgiving holidays after “having a few drinks.”  He was retaliating against the company as it was moving from Brigdeport, CT to Shelton.  Patel was fired a month before.  He intended to cause a “small hiccup” that would cause problems for a few hours, not days.  Patel deleted critical files used by the servers during boot-up. | Knocked servers offline. Crippled operations for two weeks.  Priyavrat Patel pleaded guilty and was sentenced to six months in prison followed by three years of supervised release, the first six months must be served in home confinement.  Patel has separately agreed to pay Pratt-Read $120,000 in restitution, but he may be ordered to pay additional fines.
 | https://www.risidata.com/Database/Detail/power_network_survives_virus_attack | Power Network Survives Virus Attack | 2006 | Confirmed | India | Power and Utilities | A ‘Kama Sutra’ virus attack was observed in the Indian power network. | Increased awareness | No connections to the internet without adequate firewalls in place.  The use of floppies and pen drives were banned in Windows-based computers in the ULDC system.
 | https://www.risidata.com/Database/Detail/chemical_plant_explosion | Chemical Plant Explosion | 2008 | Confirmed | United States | Chemical | An explosion and fire occurred at the Bayer Cropscience plant in West Virginia.  The explosion was caused by the runaway reaction that created extremely high heat and pressure in the residue treated which ruptured and flew 50ft in the air.  Two operators died.  There were significant lapses in the plant’s process safety management.  This includes inadequate training on new equipment and the overriding of critical safety systems which was necessary since the heater could not produce the required temperature for safe operations.   Prior to starting up, Bayer recently upgraded the computer control system for the Methomyl/Larvin unit.  The control screens of the Siemens system were completely different than the Honeywell system that was replaced. Operators were not adequately trained. Operators used a workaround to deal with the long-standing heater problem. | The failure to follow procedures, lack of training and overriding safety control systems resulted in an explosion.  Two operators died and the public was put at risk.  Eight workers reported symptoms of chemical exposure.
 | https://www.risidata.com/Database/Detail/hacker_disabled_offshore_oil_platforms | Hacker Disabled Offshore Oil Platforms | 2008 | Confirmed | United States | Petroleum | A disgruntled  employee, Mario Azar,  accessed the system that monitors the detection of pipeline leaks for three oil derricks off the Southern California coast.  He knowingly temporarily disabled the system.  He faces a maximum 10 year sentence. The FBI announced  that, on September 14, 2009, Mario Azar pleaded guilty to intentionally damaging a computer system used in interstate and foreign commerce and faced ten years in prison. His sentencing is scheduled for December 7 in the United States District Court of Los Angeles. Update 2May2011:  In April 2010, Mario Azar was sentenced to probation for 5 years.  In addition, he was ordered to pay a special assessment of $100,000 to the clerk.   Mr. Azar was also ordered to pay $50,000 in restitution and complete 200 hours of community service. | The leak detection system was shut-down for a period of time. There were no oil leaks from the derricks that are connected to the Southern California shore.
 | https://www.risidata.com/Database/Detail/blackout_in_florida | Blackout in Florida | 2008 | Confirmed | United States | Power and Utilities | A field engineer was diagnosing a malfunctioning 138 kV switch at Florida Power and Light Flagami substation in West Miami, Florida.  Without authorization, the engineer disabled two levels of relay protection.  The local primary protection and the local back-up breaker failure protection were disabled.  Standard procedure were violated.   Removing two levels of protection is not allowed.  When the switch was opened an electrical arc was generated that migrated to other energized equipment and ground  causing a major short circuit. | 680,000 customers were affected with an additional two-million plus customers affected in other parts of the state.  Several different power companies lost control of their grids.  It is estimated that as many as four million customers in Florida were affected.  Turkey Point nuclear power plant located south of Miami was affected.  The affected region ranged from Miami to Tampa, through Orlando and east to Brevard County where Cape Canaveral and the Kennedy Space Center are located. | The Florida Reliability Coordinating Council (FRCC) has made 24 recommendations.  Several have already been completed.  The first was completed by FPL. Recommendation 1: "Review and enhance as required, present policies and procedures related to removing protection systems from service when performing maintenance or troubleshooting. Note: The interim measures already implemented by FPL addressed this issue. (ref. FPART 1)"
 | https://www.risidata.com/Database/Detail/zotob_pnp_worms_hit_13_automotive_manufacturing_plants | Zotob, PnP Worms Hit 13 Automotive Manufacturing Plants | 2005 | Confirmed | United States | Automotive | Internet worm infections caused 13 DaimlerChrysler plants to go offline for up to 50 minutes stopping the assembly lines. | Assembly lines at 13 plants stopped until the systems were patched.  More than 50,000 assembly line workers were sent home.  Vehicle production was stopped for up to 50 minutes.
 | https://www.risidata.com/Database/Detail/browns_ferry_nuclear_plant_scrammed_shut_down | Browns Ferry Nuclear Plant Scrammed (Shut Down) | 2006 | Confirmed | United States | Power and Utilities | Operators manually scrammed Browns Ferry, Unit 3, following a loss of both the 3A and 3B reactor recirculation pumps. The root cause was the malfunction of the Siemens Perfect Harmony VFD controller due to excessive traffic on the plant ethernet based integrated computer system (ICS) network. From NRC report dated April 17, 2007:  The licensee determined that the root cause of the event was the malfunction of the VFD controller because of excessive traffic on the plant ICS network. | A manual scram or shutdown. | Unit 3 VFD controllers were disconnected from the plant ICS network before restart.  Unit 2 VFD controllers were also disconnected.
 | https://www.risidata.com/Database/Detail/schoolboy_hacks_into_polish_tram_system | Schoolboy Hacks into Polish Tram System | 2008 | Confirmed | Poland | Transportation | A 14-year old boy, a Polish student, hacked into the tram system which enabled him to change track points in Lodz, Poland.  Four trams were derailed. Twelve people were injured when a train derailed.  The boy built an infrared device that looked like a TV remote control that could control all the junctions on the line.  No deaths occurred.  The boy will face a special juvenile  court on charges of endangering public safety. | Four trams were derailed. Twelve people were injured when a train derailed.
 | https://www.risidata.com/Database/Detail/California_Canal_System_Hack | California Canal System Hack | 2007 | Confirmed | United States | Water/Waste Water | Michael Keehn, 61  a former electrical supervisor at the Tehama Colusa Canal Authority  was arrested following a grand jury indictment on November 15, 2007 alleging that he intentionally accessed and damaged the computer used to divert water from the Sacramento River to provide, among other things, irrigation to local farms.  He is alleged to have “intentionally caused damage without authorization to a protected computer”.  He installed unauthorized software on the TCAA’s SCADA system.   This computer operates the Tehama Colusa and Corning canals , which are owned by the United States Bureau of Reclamation.  Keehn accessed the system on or about August 15, 2007, the same day he was fired.  Keehn faces a maximum of 10 years in prison and a 3 year term of supervised release.  He pleaded not guilty to computer fraud on November 28, 2007 at his hearing.  He was sentenced to 10 years imprisonment. | The intrusion cost the TCCA more than $5,000 in damages.
 | https://www.risidata.com/Database/Detail/pennsylvania_water_company_hack | Pennsylvania Water Company Hack | 2006 | Confirmed | United States | Water/Waste Water | A hacker used the internet to access an employee’s laptop.  This enabled the hacker to use the employee’s remote access to install a virus and spyware in the water plant computer system. | The company eliminated remote access to the system.  All passwords were changed.
 | https://www.risidata.com/Database/Detail/georgia_nuclear_power_plant_shutdown | Georgia Nuclear Power Plant Shutdown | 2008 | Confirmed | United States | Power and Utilities | Hatch Nuclear Power Plant was forced to shutdown for 48 hours after a contractor updated software on a computer that was on the plant’s business network.  The computer was used to monitor chemical and diagnostic data from one of the facility’s  primary control systems.  The software was designed to synchronize data on both systems.  When the updated computer rebooted, it reset the data on the control system to interpret the lack of data as a drop in water reservoirs that cool the plant’s radioactive nuclear rods.  The safety system triggered a shutdown. The engineer was not aware that the control system would be synchronized as well and that a reboot would reset the control system. | Power plant shutdown for 48 hours. | Removed all network connections between affected servers.
 | https://www.risidata.com/Database/Detail/power_plant_security_information_leaked_onto_internet | Power Plant Security Information Leaked Onto Internet | 2006 | Confirmed | Japan | Power and Utilities | Security data on a thermal power plant was leaked onto the Internet from a virus-infected personal computer belonging to a 40-year-old employee of the plant’s security firm through a file-sharing program called Share. Compromised data included the locations of various facilities including the control room, instrument panel room and boilers. Manuals on how to deal with unconfirmed reports of intruders in the plant, as well as a list of the names and home addresses of the security firm’s employees and other personal data on guards were also compromised.
 | https://www.risidata.com/Database/Detail/Windows_Compiler_Causes_PLC_Code_to_Crash_PLC | Windows Compiler Causes PLC Code to Crash PLC | 2001 | Likely But Unconfirmed | United Kingdom | Food & Beverage | When making minor changes to a PLC program the PLC started to operate the plant in the wrong way. Valves opened at the wrong time, the batch sequence changed and the operator was unable to control his plant. The PLC in question used a number of subroutines that over time had changed. The PLC manufacturer’s software used the Microsoft Windows compiler to compile the code; however, this compiler failed to pick up the fact that there were too many variables being passed to the subroutine. | The organisation was unable to change the program on the PLC for a number of months and the problem was discovered by accident. An additional 3 PLCs had a similar style of programming and were experiencing the same problem. | Working with the PLC manufacturer, a patch to their software was installed to prevent this from happening again.
 | https://www.risidata.com/Database/Detail/Loss_of_Network_Traffic_on_PCN | Loss of Network Traffic on PCN | 2004 | Likely But Unconfirmed | United States | Food & Beverage | The business and control networks were VLANed on the switch. A problem on the business system network resulted in the loss of network traffic on the control system. No report of the actual cause of the incident was filed, so the root cause was not identified to others working outside of this site. | Production was severely effected during this period.
 | https://www.risidata.com/Database/Detail/Wrong_Code_Downloaded_to_PLC_Causes_Plant_to_Shutdown | Wrong Code Downloaded to PLC Causes Plant to Shutdown | 1997 | Likely But Unconfirmed | United Kingdom | Food & Beverage | When making minor changes to a PLC program, the PLC and Slave PLC were loaded with the new program and made the Master. The new Slave, previously the Master, was also loaded halfway through this download. The Master PLC stopped working and the plant was shut down. On a previous training course, the instructions were always to download to both units. However, a colleague, unaware of what had been taught on the training course, had discovered that it was possible to run different programs on the Master and Slave on a previous occasion, as long as they were not too dissimilar. | The plant was shutdown for 1 | All new starters were told of the procedure used when downloading software and why. This was done only after they had been on their formal training with the software supplier. The software supplier was told of the problem and how the problem would be avoided in the future. This was incorporated into their training. Additionally, the organisation developed a policy of not having more than one person working on a PLC at the same time.  At the time of the incident there were two developers working on the same PLC (working on different priority jobs). This caused confusion when trying to troubleshoot the problem.
 | https://www.risidata.com/Database/Detail/New_Serial_Communications_Line_Disrupts_Network | New Serial Communications Line Disrupts Network | 1997 | Likely But Unconfirmed | United Kingdom | Food & Beverage | While a project team was connecting a new serial communication between two PLCs, the main site network connected to the master of these two PLCs was interrupted. Both control and view of the running plant was interrupted. The running plant had its network disconnected while the serial connection was remade. | The operator lost visibility of the plant for about 30 minutes. | The exact cause was not discovered. The project and production teams developed a procedure so that when new kit was being connected everybody was aware of the situation and the appropriate individuals would be available should problems occur.
 | https://www.risidata.com/Database/Detail/Software_Vendor_Patch_Crashes_SCADA_System | Software Vendor Patch Crashes SCADA System | 2005 | Likely But Unconfirmed | United Kingdom | Food & Beverage | A software patch was supplied from the vendor to correct an existing problem. This patch caused the SCADA server to fail, stopping the entire system. Operations of 3 plants were carried out on the one working PC, a stand-alone backup PC that had yet to be patched. The system was restore to full operation by inserting a hard disk that was removed from the server prior to the patch being installed, this was the first time that the disk had been removed. The software re-issued the patch with revised instructions detailing it was not to be used on the SCADA server. | Production was severely effected during this period, and the on-call engineer was called to the site. An additional operator was also called in to help the existing personnel run the site using the one PC. | Each patch installation is scrutinised and unless required urgently, the patch is not installed until it is at least a month old. All future patch installations require the second hard disk on the server to be removed and replaced with a blank one (mirrored disks are used). The period that the server is considered to be under test for was extended to a month. This means that the disk removed cannot be used elsewhere until the end of this period.
 | https://www.risidata.com/Database/Detail/Partial_Loss_of_PLC_Program | Partial Loss of PLC Program | 2005 | Likely But Unconfirmed | United Kingdom | Food & Beverage | There had been reports of problems with a control loop on the system for a number of weeks. Upon investigation, it was discovered that it was possible to delete a sequence without any notification when working on line. It was assumed that this was what had happened and it was also discovered that it was not the only sequence deleted. | Unable to correctly control the specification of a final product. | The Control team were all informed of the possibility of this happening and the supplier was informed that a modification should be made to their software.
 | https://www.risidata.com/Database/Detail/accidental_remote_control | Accidental Remote Control | 2000 | Likely But Unconfirmed | Sweden | General Manufacturing | A simulation device located in the UK and identical to that installed in the running plant was also given the same communication parameters as were used on the plant device. Eventually the device in the UK assumed control. | Approximately 4 hours production loss. | Isolation of the process network. Introduction of process network firewall. Secure site access control system.
 | https://www.risidata.com/Database/Detail/Infected_Laptop_Infects_SCADA_Network | Infected Laptop Infects SCADA Network | 2004 | Likely But Unconfirmed | United Kingdom | Food & Beverage | A contracting company attached a laptop to a SCADA system running a pilot plant. After a while, the SCADA started exhibiting strange behaviours and upon investigation it was discovered that the server was infected. There was no anti-virus on the laptop and when it was scanned it was discovered it was infected by a large number of viruses. | The pilot plant had to be shutdown until the problem was resolved. | All laptops are scanned for viruses before being allowed to connect to the system. Some sites went further and banned external laptops from being connected and requested that all files be sent via email and scanned through the email and file systems before being loaded onto the SCADA system.
 | https://www.risidata.com/Database/Detail/Virus_Infection_On_DCS | Virus Infection On DCS | 1999 | Likely But Unconfirmed | United Kingdom | Food & Beverage | Data on the HMI server was being corrupted, this data caused the system to slow and eventually stop. On investigation, it was discovered that the R&T server (in the same domain) had been infected and was totally unusable - unlike the control servers which were partially damaged. The virus was identified as Remote Explorer. The virus was cleaned over a period of a week and anti-virus was then added to the control system and R&T servers. | A group of 7 people spent the week cleaning, rechecking, re-cleaning and installing anti-virus software. This prevented the normal day to day work being carried out and delayed a commissioning job of a new plant. There were problems with ownership of the problem with a recently outsourced IT team. Later conversations with other companies in the group indicated that this had been a wide spread issue on the corporate network but this information was never passed on. | Installed anti-virus software and developed a relationship between the Control and IT departments.
 | https://www.risidata.com/Database/Detail/trojan_found_on_scada_server | Trojan Found on SCADA Server | 2006 | Confirmed | Russia | General Manufacturing | A redundant SCADA server pair were having communication issues between themselves and other SCADA servers.  After several modifications and a rebuild, the AV was enabled and it located numerous instances of the Trojan ‘Generic Backdoor.k' on one machine. This was removed and all other devices on the LAN were tested. No other machines were infected. This proved not to be the cause of the communication problems; however, the USB hard disk used for taking hard drive images was tested and also found to have the same Trojan. It is not clear if this was the source of the Trojan as the plant had just been commissioned and removable media had been used during this period for making modifications. | It took time for several engineers to test all machines. | The use of removable media is now banned on site and the USB hard disk for hard disk images was tested before being used.
 | https://www.risidata.com/Database/Detail/attempted_cover-up_of_sewage_spillage | Attempted Cover-Up of Sewage Spillage | 2005 | Confirmed | Australia | Water/Waste Water | A sewerage spill officially estimated at slightly less than 3ML discharged into a nearby creek. The engineer in charge was advised of the spill by the Electrical Supervisor and then his supervisor by telephone phone and he performed a hardcopy backup of the operational trend data containing level information and the derived flow information. The trends indicated a significant reduction in normal levels and derived flow. The derived flow figure value was approximately 4 times higher than actual after checking the site post event. The flow meter from this site does not have a pulsed output and was not working prior or during the incident due to a tripped circuit breaker for this unit. The flow value did not completely stop during the event and the pumps ran lower hours than normal. The results of pump stations operations staff investigations discovered that a pump station inlet control valve after being routinely serviced failed normal operation and closed partially after service personnel left site. The valve opening was subsequently blocked by a large piece of wood and cloth like material (referred to as ragging) completely blocking the valve entry point to the pump station. At least two Senior management discussed the situation while in a highly panicked and irrational state as they were in fear of the possible consequences of the event (e.g. loss of jobs, personal prosecution and criminal charges arising from the spill because of only recently having to deal with two serious spills). The engineer in charge received a callout from the on call system operator complaining of being locked out of terminal even though he was able to dial in. the engineer discovered in the event log that his supervisor was connected via the corporate LAN and was looking at system and had locked out the terminal. The on call system operator rang the engineer in charge and advised him that the terminal had become available again. The engineer in charge checked the I/O Server System Trend Data and it was intact at this time. The engineer in charge's supervisor entered the water treatment plant (the location of the telemetry systems servers). This was logged in the electronic access control security system log. A short time later the engineer in charge performed a daily system check which basically consists of turning the monitor on the I/O server and looking at the system for any system related problems. He noticed that the “Trashcan” was full and immediately asked the Electrical Supervisor to observe along side while he opened the contents of the trashcan and any subsequent checking. The trashcan contained trend data. He then logged into the system and checked via the kernel menu for the system uptime which was less than 4 hours. The engineer in charge rang the General Manager of this discovery and asked him as to what he required him to do. Within five minutes his supervisor rang and advised that “he had attended the site earlier” and that he would “come to the site and undelete and restore the data on the server”. The engineer in charge’s supervisor arrived on site using his access card to gain entry to the complex. He then admitted to altering the system earlier in the morning and that he was going to attempt to reinsert the missing or altered data. If the SCADA system and Historical Data system was not altered the data would have aided in proving the causation of the event.  The system could not be shut down immediately following the incident and this was not authorised until 5 days after the incident and took 24 hours to implement. | Three staff subsequently resigned resulting in an increased workload. The EPA is currently assessing if they are going to proceed with legal process. | Strategic plans for SCADA systems are to include security/risk management frameworks. Audit computer system to secure and lock down the system as much as is possible and preserve the data of the event as best as is practical. Implemented more automatic copying of data bases to more locations that can be copied without causing the systems to crash. Changed all Admin Passwords on system and on individual machines. More automation of backup systems. A software specification has been written to address basic user system requirements features of the software include present Process Control best industry practice. This will allow further layers to be put into place to between the application administration and the operating systems administration. This specification will also provide a programming environment to be able to easily perform the flow calculations alarms to meet this item as necessary.
 | https://www.risidata.com/Database/Detail/remote_software_upgrade_causes_loss_of_control_view | Remote Software Upgrade Causes Loss of Control & View | 2006 | Confirmed | United Kingdom | Transportation | ABB performed a remote, unplanned, and unannounced software upgrade to the Power Control Centre between 10am and 11am resulting in a 22 minute loss of control and view of all substations across the network. | Loss of control and view of all substations across the network for 22 minutes. | Decommissioned the undocumented ISDN line from the control centre.
 | https://www.risidata.com/Database/Detail/cancer_treatment_delayed_by_virus | Cancer Treatment Delayed by Virus | 2006 | Confirmed | United Kingdom | Other | Two out of four linear accelerators were out of action for two days.  The machines, which did not have anti-viral software installed, were hit by the virus when they were connected to the hospital computer system. The machines are connected to the computer system to allow the hospital to send electronic images from radiotherapy around the hospital, instead of using film. | More than 80 cancer patients have had their radiotherapy delayed . Staff had to run extra clinics over the weekend to clear the backlog. | The hospital said it would be discussing the situation with the manufacturer of the linear accelerators.
 | https://www.risidata.com/Database/Detail/penetration_test_locks-up_gas_utility_scada_system | Penetration Test Locks-Up Gas Utility SCADA System | 2002 | Confirmed | Unknown | Other | A gas utility hired an IT security consulting company to conduct penetration testing on their corporate IT network and carelessly ventured into a part of the network that was directly connected to the SCADA system causing it to lock-up. | Loss of service to the customer base for 4 hours.
 | https://www.risidata.com/Database/Detail/Ping_Sweep_Causes_PCS_System_to_Hang | Ping Sweep Causes PCS System to Hang | 1998 | Confirmed | Unknown | Other | On a PCS network, a ping sweep was being performed to identify all hosts that were attached to the network, for inventory purposes, and it caused a system controlling the creation of integrated circuits in the fabrication plant to hang. | The destruction of $50K worth of wafers.
 | https://www.risidata.com/Database/Detail/ping_sweep_causes_inappropriate_control_of_a_9_foot_robotic_arm | Ping Sweep Causes Inappropriate Control of a 9 Foot Robotic Arm | 1998 | Confirmed | Unknown | Other | While a ping sweep was being performed on an active SCADA network that controlled 9-foot robotic arms, it was noticed that one arm became active and swung 180 degrees. The controller for the arm was in standby mode before the ping sweep was initiated.
 | https://www.risidata.com/Database/Detail/Server_Out_of_Memory | Server Out of Memory | 2006 | Confirmed | Italy | Other | The process providing report management replicate itself 330 times because it was unable to find a value from an off-line DCS. The server went out of physical and swap memory. | Unknown. | Report the incident to Siemens R&D department and wait for a patch.
 | https://www.risidata.com/Database/Detail/Sasser_Infection_from_the_Enterprise_Network | Sasser Infection from the Enterprise Network | 2004 | Confirmed | Unknown | Other | The Korgo worm impacted 20 workstations (view only terminals) as the result of a corporate patch distribution problem. | Unknown.
